{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67488b4",
   "metadata": {},
   "source": [
    "# Work with Text and Images\n",
    "\n",
    "You need to run in the terminal:\n",
    "```bash\n",
    "pip install matplotlib\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install -U torch torchvision\n",
    "pip install -U \"transformers>=4.41.0\"\n",
    "pip install -U qwen-vl-utils\n",
    "pip install -U pillow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be24150",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c67c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add auto reload for notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import base64\n",
    "\n",
    "from conversational_toolkit.chunking.pdf_chunker import PDFChunker\n",
    "\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.embeddings.clip import CLIPEmbeddings\n",
    "from conversational_toolkit.embeddings.qwen_vl import Qwen3VLEmbeddings\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import (\n",
    "    CompositeVectorStoreRetriever,\n",
    ")\n",
    "from conversational_toolkit.llms.base import LLMMessage, MessageContent, Roles\n",
    "from conversational_toolkit.llms.openai import OpenAILLM\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe4a9d",
   "metadata": {},
   "source": [
    "# Parse document and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510763eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = PDFChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c54450",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker.make_chunks(\n",
    "    \"../../data/EPD_cardboard_redbox_cartonpallet.pdf\",\n",
    "    write_images=True,\n",
    "    image_path=\"../../data/tmp_images/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {chunk.title} ({chunk.mime_type})\")\n",
    "    if chunk.mime_type.startswith(\"image/\"):\n",
    "        image_data = base64.b64decode(chunk.content)\n",
    "\n",
    "        image = plt.imread(io.BytesIO(image_data), format=chunk.mime_type.split(\"/\")[1])\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec53ef",
   "metadata": {},
   "source": [
    "# Embed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_model = SentenceTransformerEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# if os.path.exists(\"../../db/db_text_tmp/\"):\n",
    "#    shutil.rmtree(\"../../db/db_text_tmp/\")\n",
    "text_vector_store = ChromaDBVectorStore(db_path=\"../../db/db_text_tmp/\")\n",
    "\n",
    "text_chunks = [c for c in chunks if c.mime_type.startswith(\"text/\")]\n",
    "\n",
    "embeddings = await text_embedding_model.get_embeddings([c.content for c in text_chunks])\n",
    "\n",
    "await text_vector_store.insert_chunks(chunks=text_chunks, embedding=embeddings)\n",
    "\n",
    "print(sum(len(e) for e in embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the amount of PBT/vPvB substances ?\"\n",
    "\n",
    "query_embedding = await text_embedding_model.get_embeddings(query)\n",
    "\n",
    "results = await text_vector_store.get_chunks_by_embedding(query_embedding, top_k=5)\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i}: {result.title} ({result.mime_type})\")\n",
    "\n",
    "print(results[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60002d",
   "metadata": {},
   "source": [
    "# Embed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf40db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embedding_model = CLIPEmbeddings()\n",
    "image_embedding_model = Qwen3VLEmbeddings()\n",
    "\n",
    "# if os.path.exists(\"../../db/db_image_siglip_tmp/\"):\n",
    "#     shutil.rmtree(\"../../db/db_image_siglip_tmp/\")\n",
    "\n",
    "image_vector_store = ChromaDBVectorStore(db_path=\"../../db/db_image_siglip_tmp/\")\n",
    "\n",
    "image_chunks = [c for c in chunks if c.mime_type.startswith(\"image/\")]\n",
    "\n",
    "embeddings = await image_embedding_model.get_embeddings([c for c in image_chunks])\n",
    "\n",
    "await image_vector_store.insert_chunks(chunks=image_chunks, embedding=embeddings)\n",
    "\n",
    "print(sum(len(e) for e in embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A carboard box on a pallet\"\n",
    "\n",
    "query_embedding = await image_embedding_model.get_text_embeddings(query)\n",
    "\n",
    "results = await image_vector_store.get_chunks_by_embedding(query_embedding, top_k=5)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i}: {result.title} ({result.mime_type})\")\n",
    "    image_data = base64.b64decode(result.content)\n",
    "\n",
    "    image = plt.imread(io.BytesIO(image_data), format=result.mime_type.split(\"/\")[1])\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f78f601",
   "metadata": {},
   "source": [
    "# Create Composite Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the amount of PBT/vPvB substances ?\"\n",
    "\n",
    "composite_retriever = CompositeVectorStoreRetriever(\n",
    "    embedding_models=[text_embedding_model, image_embedding_model],\n",
    "    vector_stores=[text_vector_store, image_vector_store],\n",
    "    top_k=[3, 1],\n",
    ")\n",
    "\n",
    "results = await composite_retriever.retrieve(query)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    # if text print it\n",
    "    print(f\"Result {i} - type: {result.mime_type}\")\n",
    "\n",
    "    if result.mime_type.startswith(\"text/\"):\n",
    "        print(result.content)\n",
    "    elif result.mime_type.startswith(\"image/\"):\n",
    "        image_data = base64.b64decode(result.content)\n",
    "\n",
    "        image = plt.imread(\n",
    "            io.BytesIO(image_data), format=result.mime_type.split(\"/\")[1]\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4062625a",
   "metadata": {},
   "source": [
    "# Send Image to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILLM()\n",
    "\n",
    "image_as_base64 = results[-1].content\n",
    "image_data = base64.b64decode(image_as_base64)\n",
    "image = plt.imread(io.BytesIO(image_data), format=results[-1].mime_type.split(\"/\")[1])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb824ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    LLMMessage(\n",
    "        content=[\n",
    "            MessageContent(\n",
    "                type=\"input_text\",\n",
    "                text=\"What is in this image?\",\n",
    "            ),\n",
    "            MessageContent(\n",
    "                type=\"input_image\",\n",
    "                image_url=image_as_base64,\n",
    "            ),\n",
    "        ],\n",
    "        role=Roles.USER,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = await llm.generate(test_messages)\n",
    "\n",
    "print(\"\\n\\nAnswer content:\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb28a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_stream = llm.generate_stream(test_messages)\n",
    "\n",
    "result = \"\"\n",
    "async for chunk in answer_stream:\n",
    "    result += chunk.content[0].text if chunk.content else \"\"\n",
    "\n",
    "print(\"\\n\\nAnswer content:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de9d23",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
