{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d814a8de",
   "metadata": {},
   "source": [
    "# Improving RAG Retrieval (Feature Track 4)\n",
    "\n",
    "Coded by Adelene Lai (Econetta AG)\n",
    "\n",
    "26.02.2026\n",
    "\n",
    "There are several ways to retrieve chunks:\n",
    "* Dense - distance based on embedding of query\n",
    "* Sparse - distance based on (key)words\n",
    "* Hybrid - weighted sum of sparse + dense\n",
    "\n",
    "Top-k chunks are those closest to the query using ANN.\n",
    "\n",
    "Let's compare the retrievers for different questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7db1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644a7c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-02-26 15:38:05.687125535 [W:onnxruntime:Default, device_discovery.cc:211 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:91 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n",
      "/home/alai/projects/myfork/sme-kt-zh-collaboration-rag/rag_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "from conversational_toolkit.retriever.bm25_retriever import BM25Retriever\n",
    "from conversational_toolkit.retriever.hybrid_retriever import HybridRetriever\n",
    "from conversational_toolkit.retriever.reranking_retriever import RerankingRetriever\n",
    "\n",
    "# from conversational_toolkit.retriever.XYZ import XYZRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# Choose your LLM backend: \"ollama\" (local, requires `ollama serve`) or \"openai\" (requires OPENAI_API_KEY)\n",
    "BACKEND = \"openai\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", or \"openai\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fae6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:38:13.411 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 - Chunking 5 files from /home/alai/projects/myfork/sme-kt-zh-collaboration-rag/data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:38:13.696 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:217 -   ART_internal_procurement_policy.pdf: 12 chunks\n",
      "2026-02-26 15:38:13.893 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:217 -   ART_logylight_incomplete_datasheet.pdf: 6 chunks\n",
      "2026-02-26 15:38:14.004 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:217 -   ART_product_catalog.pdf: 7 chunks\n",
      "2026-02-26 15:38:14.010 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:217 -   ART_product_overview.xlsx: 1 chunks\n",
      "2026-02-26 15:38:14.121 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:217 -   ART_relicyc_logypal1_datasheet_2021.pdf: 5 chunks\n",
      "2026-02-26 15:38:14.121 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:221 - Done, 31 chunks total\n",
      "2026-02-26 15:38:16.578 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-26 15:38:16.706 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:265 - Vector store already contains 31 chunks — skipping embedding.\n",
      "2026-02-26 15:38:16.707 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:140 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-26 15:38:16.726 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "chunks = load_chunks(5)\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vector_store = await build_vector_store(chunks, embedding_model, reset=False)\n",
    "llm = build_llm(\"openai\", model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deccde0",
   "metadata": {},
   "source": [
    "## Try with normal VectorStoreRetriever that does dense retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72db015e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inspect_retrieval() got multiple values for argument 'top_k'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m QUERY = \u001b[33m\"\u001b[39m\u001b[33mWhat materials is the Logypal 1 pallet made from?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43minspect_retrieval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mQUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETRIEVER_TOP_K\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: inspect_retrieval() got multiple values for argument 'top_k'"
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(\n",
    "    QUERY, vector_store, embedding_model, top_k=RETRIEVER_TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3eab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:38:33.878 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:140 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-26 15:38:33.898 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n",
      "2026-02-26 15:38:33.899 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_agent:349 - RAG agent ready (top_k=5  query_expansion=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent assembled.\n"
     ]
    }
   ],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability and product compliance for PrimePack AG.\\n\\n\"\n",
    "    \"You will receive document excerpts relevant to the user's question. Produce the best possible answer using only the information in those excerpts.\"\n",
    ")\n",
    "agent = build_agent(\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    "    retriever=VectorStoreRetriever(\n",
    "        embedding_model=embedding_model,\n",
    "        vector_store=vector_store,\n",
    "        top_k=RETRIEVER_TOP_K,\n",
    "    ),  # 0 = no expansion; see Feature Track 3 for more\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a0753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:38:36.913 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:366 - Query: 'What materials is the Logypal 1 pallet made from?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Query: 'What materials is the Logypal 1 pallet made from?'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:38:37.191 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-26 15:38:38.638 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:369 - Answer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logypal 1 pallet is made from 100% post-consumer recycled plastic, primarily sourced from end-of-life agricultural packaging, such as silage film, and industrial packaging waste.\n",
      "Sources (5):\n",
      "  'ART_relicyc_logypal1_datasheet_2021.pdf'  |  '## Overview'\n",
      "  'ART_logylight_incomplete_datasheet.pdf'  |  '## Product Overview'\n",
      "  'ART_logylight_incomplete_datasheet.pdf'  |  '## Material Composition'\n",
      "  'ART_relicyc_logypal1_datasheet_2021.pdf'  |  '## Material Composition (2021)'\n",
      "  'ART_logylight_incomplete_datasheet.pdf'  |  '# Relicyc LogyLight — Sustainability Information Sheet'\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6b8d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m agent = build_agent(\n\u001b[32m      2\u001b[39m     llm=llm,\n\u001b[32m      3\u001b[39m     top_k=RETRIEVER_TOP_K,\n\u001b[32m      4\u001b[39m     system_prompt=SYSTEM_PROMPT,\n\u001b[32m      5\u001b[39m     number_query_expansion=\u001b[32m0\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     retriever=\u001b[43mBM25Retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETRIEVER_TOP_K\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRAG agent assembled.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/myfork/sme-kt-zh-collaboration-rag/conversational-toolkit/src/conversational_toolkit/retriever/bm25_retriever.py:36\u001b[39m, in \u001b[36mBM25Retriever.__init__\u001b[39m\u001b[34m(self, vector_store, top_k)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# self.corpus = corpus\u001b[39;00m\n\u001b[32m     35\u001b[39m chunks = vector_store.collection.get(include=[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m tokenized = [\u001b[38;5;28mself\u001b[39m._tokenize(\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m._bm25 = BM25Okapi(tokenized)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "agent = build_agent(\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    "    retriever=BM25Retriever(vector_store=vector_store, top_k=RETRIEVER_TOP_K),\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b44dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141b234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bb37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605c5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
