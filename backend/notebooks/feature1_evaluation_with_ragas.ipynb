{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p1-title-md",
   "metadata": {},
   "source": [
    "# Feature Track 1: Evaluation & Validation\n",
    "\n",
    "---\n",
    "\n",
    "Shipping a RAG system without systematic evaluation is like navigating without instruments. The pipeline may *seem* to work on the queries you tested by hand, but you have no way to know where it breaks, how often, or whether a change you made helped or hurt.\n",
    "\n",
    "**Evaluation closes the feedback loop:**\n",
    "\n",
    "```\n",
    "Change a parameter  ──►  Measure quantitatively  ──►  Decide based on data\n",
    "```\n",
    "\n",
    "**Prerequisite:** Run `feature0_baseline_rag.ipynb` Steps 1–2 first to build the vector store.\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| Feature 0 | Working baseline prototype |\n",
    "| **Feature Track 1 (this notebook)** | Quantitative evaluation |\n",
    "| Feature Track 2 | Reliable, structured outputs |\n",
    "| Feature Track 3 | Better retrieval strategies |\n",
    "| Feature Track 4 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-why-md",
   "metadata": {},
   "source": "---\n\n## Foundation\n\n### Why Systematic Evaluation?\nWithout metrics, you are forced to manually re-read answers for a handful of test queries and guess whether a change helped or hurt. With metrics you run the evaluation suite and get a number — one you can track across pipeline changes and use to justify decisions.\n\n**Concrete example from Feature 0:** The baseline RAG sometimes described a non-existent product as if it exists, cited a superseded GWP figure, or reported an unverified CO₂ reduction without flagging it. These are exactly the queries that matter for compliance. How often does this happen? After every change to the pipeline, you need an answer."
  },
  {
   "cell_type": "markdown",
   "id": "p1-pipeline-md",
   "metadata": {},
   "source": "---\n\n### The RAG Pipeline\n\nEach arrow is a potential failure point. Evaluation targets a specific stage so you can isolate *where* a problem is.\n\n```\nIngestion (run once)\n  Documents ──► [1] Chunker ──► [2] Embedder ──► [3] Vector DB\n\nQuerying (every user question)\n  User query ──► [2] Embedder ──► [3] Retriever ──► Top-k Chunks\n                                                          │\n                                                   [4] LLM + Prompt\n                                                          │\n                                                   Answer + Sources\n```\n\n| Step | If it fails |\n|---|---|\n| [1] Chunking | Context split mid-fact; tables broken |\n| [2] Embedding | Wrong chunks returned |\n| [3] Vector search | Relevant chunks not retrieved |\n| [4] Generation | Hallucination; ignores context; incomplete answer |"
  },
  {
   "cell_type": "markdown",
   "id": "p1-map-md",
   "metadata": {},
   "source": "---\n\n### Stage-by-Stage Evaluation Map\n\n| Stage | What to measure |\n|---|---|\n| **Ingestion / Parsing** | Text completeness; table structure preserved |\n| **Chunking** | Chunk size distribution; chunks exceeding token limit |\n| **Embedding** | Similarity gap between relevant and irrelevant chunks |\n| **Vector search** | Fraction of queries where the correct chunk appears in top-k |\n| **Retrieved context** | Relevance and completeness of retrieved chunks |\n| **Faithfulness** | Fraction of answer claims supported by the retrieved context |\n| **Answer relevance** | Whether the answer addresses the actual question |\n| **Answer correctness** | Factual accuracy vs. known ground truth |"
  },
  {
   "cell_type": "markdown",
   "id": "p1-ragas-md",
   "metadata": {},
   "source": "---\n\n## RAGAS\n\n[RAGAS](https://docs.ragas.io) (*Retrieval Augmented Generation Assessment*) is an open-source Python library for evaluating RAG pipelines, widely adopted in the LLM/RAG ecosystem.\n\n#### How it works internally\nRather than asking a judge LLM \"rate this answer 0–5\", RAGAS decomposes the answer into individual atomic claims:\n\n```\nAnswer: \"The Logypal 1 GWP is 3.2 kg CO₂e, verified by Bureau Veritas.\"\n\n  Claim 1: \"GWP is 3.2 kg CO₂e\"          → supported by context?  ✓\n  Claim 2: \"verified by Bureau Veritas\"  → supported by context?  ✓\n\n  Faithfulness = 2 supported / 2 total = 1.0\n```\n\nThis catches partial hallucination — e.g. a correct figure with a fabricated verifier name.\n\n#### Metrics overview\n\n| Metric | Ground truth needed? | What it catches |\n|---|---|---|\n| `Faithfulness` | No | Claims not supported by the retrieved context |\n| `AnswerRelevancy` | No | Off-topic or evasive answers |\n| `ContextPrecision` | **Yes** (reference answer) | Irrelevant chunks ranked above relevant ones |\n| `ContextRecall` | **Yes** (reference answer) | Facts needed to answer that are missing from the retrieved context |\n| `AnswerCorrectness` | **Yes** (reference answer) | Wrong or missing facts vs. the reference answer |\n\nThe first two metrics are **free to run on any query** — no labelling effort required. The last three require a **test set**: a curated list of (query, reference answer) pairs.\n\n#### Strengths and weaknesses\n- Standardised, reproducible metrics widely used in industry\n- `Faithfulness` and `AnswerRelevancy` require zero labelling effort\n- Needs a capable judge LLM (defaults to OpenAI) — costs ~3 LLM calls per sample per metric\n- LLM judge has its own biases; metrics are proxies, not absolute ground truth"
  },
  {
   "cell_type": "markdown",
   "id": "p2-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### First Look at RAGAS\n",
    "\n",
    "#### Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` installed in editable mode. Vector store must already exist -> run `feature0_baseline_rag.ipynb` Steps 1–2 first.\n",
    "\n",
    "RAGAS uses OpenAI as its judge LLM by default ->`OPENAI_API_KEY` must be set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 00:53:33.228 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-24 00:53:33.240 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:135 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-24 00:53:33.263 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n",
      "2026-02-24 00:53:33.263 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_agent:332 - RAG agent ready (top_k=5  query_expansion=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model : sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store    : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "RAG agent LLM   : openai\n",
      "RAGAS judge LLM : gpt-4o-mini (OpenAI)\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper  # type: ignore[import-untyped]\n",
    "from ragas.metrics import (  # type: ignore[attr-defined]\n",
    "    Faithfulness as RagasFaithfulness,\n",
    "    AnswerRelevancy as RagasAnswerRelevancy,\n",
    ")\n",
    "\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.evaluation import Evaluator\n",
    "from conversational_toolkit.evaluation.adapters import evaluate_with_ragas\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    EMBEDDING_MODEL,\n",
    "    VS_PATH,\n",
    "    SYSTEM_PROMPT,\n",
    "    build_llm,\n",
    "    build_agent,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "_secret_path = pathlib.Path(\"/secrets/OPENAI_API_KEY\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ and _secret_path.exists():\n",
    "    os.environ[\"OPENAI_API_KEY\"] = _secret_path.read_text().strip()\n",
    "\n",
    "RETRIEVER_TOP_K = 5\n",
    "BACKEND = \"openai\"  # \"ollama\" or \"openai\"\n",
    "# Note: RAGAS uses OpenAI for its judge LLM regardless of BACKEND above.\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError('Set BACKEND to \"ollama\" or \"openai\" before running.')\n",
    "\n",
    "# RAG pipeline\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n",
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vs,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    ")\n",
    "\n",
    "# AnswerRelevancy internally calls embed_query() / embed_documents() to compare generated questions against the original query. langchain_openai.OpenAIEmbeddings implements this interface and is accepted directly by ragas.evaluate().\n",
    "ragas_embeddings = LangChainOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# RAGAS defaults to max_tokens=3072 for its judge LLM. Long answers with many atomic claims overflow this limit mid-JSON, causing \"output is incomplete\" errors. Wrap ChatOpenAI with a higher limit and pass it explicitly to evaluate_with_ragas().\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=8192)\n",
    ")\n",
    "\n",
    "print(f\"Embedding model : {EMBEDDING_MODEL}\")\n",
    "print(f\"Vector store    : {VS_PATH}\")\n",
    "print(f\"RAG agent LLM   : {BACKEND}\")\n",
    "print(\"RAGAS judge LLM : gpt-4o-mini (OpenAI)\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d8fdd",
   "metadata": {},
   "source": "---\n\n### Part 1: Metrics Without a Test Set\n\nThese two metrics need only the query and the system's response — **no ground truth required**:\n\n- **Faithfulness**: are all claims in the answer supported by the retrieved context?\n- **AnswerRelevancy**: does the answer directly address the question?\n\nThe `evaluate_with_ragas()` adapter converts our `EvaluationSample` objects to RAGAS format, runs the judge LLM, and returns an `EvaluationReport`.\n\n*Takes ~2–3 minutes: RAGAS makes multiple judge LLM calls per sample.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 5 evaluation samples (runs the RAG agent once per query)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 00:54:28.595 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-24 00:54:30.287 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-24 00:54:33.394 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-24 00:54:38.180 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-24 00:54:41.989 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 5 samples built.\n",
      "\n",
      "Running RAGAS: Faithfulness + AnswerRelevancy (~2-3 min)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43bc77e98df452ba4516a49aa9b9301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────\n",
      "Samples evaluated : 5\n",
      "────────────────────────────────────────\n",
      "faithfulness            0.885\n",
      "answer_relevancy        0.377\n",
      "────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Does PrimePack AG offer a product called the Lara Pallet?\",\n",
    "    \"Which products in the portfolio have a third-party verified EPD?\",\n",
    "    \"Can the 68% CO2 reduction claim for tesapack ECO (product 50-102) be included in a customer sustainability response?\",\n",
    "    \"Are any tape products confirmed to be PFAS-free?\",\n",
    "    \"Which suppliers are not yet compliant with the EPD requirement by end of 2025?\",\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Building {len(queries)} evaluation samples (runs the RAG agent once per query)...\"\n",
    ")\n",
    "samples = await Evaluator.build_samples_from_agent(agent=agent, queries=queries)\n",
    "print(f\"Done. {len(samples)} samples built.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running RAGAS: Faithfulness + AnswerRelevancy (~2-3 min)\\n\")\n",
    "\n",
    "report_basic = evaluate_with_ragas(\n",
    "    samples=samples,\n",
    "    metrics=[\n",
    "        RagasFaithfulness(),  # type: ignore[call-arg]\n",
    "        RagasAnswerRelevancy(strictness=1),  # type: ignore[call-arg]\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"─\" * 40)\n",
    "print(f\"Samples evaluated: {report_basic.num_samples}\")\n",
    "print(\"─\" * 40)\n",
    "for metric_name, score in report_basic.summary().items():\n",
    "    print(f\"{metric_name:<22}  {score:.3f}\")\n",
    "print(\"─\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07af8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample scores (F = Faithfulness, A = AnswerRelevancy)\n",
      "\n",
      "#      F     A   query                                      response\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "1    1.00  0.99 Does PrimePack AG offer a product called.. No, PrimePack AG does not offer a produc..\n",
      "2    0.62  0.00 Which products in the portfolio have a t.. Based on the information provided, the p..\n",
      "3    1.00  0.00 Can the 68% CO2 reduction claim for tesa.. Based on the information provided, the 6..\n",
      "4    1.00  0.00 Are any tape products confirmed to be PF.. As of now, there are no confirmed tape p..\n",
      "5    0.80  0.90 Which suppliers are not yet compliant wi.. As of January 2025, the following suppli..\n"
     ]
    }
   ],
   "source": [
    "# Per-sample breakdown: find which queries score best / worst so we know where to focus improvement efforts.\n",
    "\n",
    "import math\n",
    "\n",
    "f_result = next(\n",
    "    (r for r in report_basic.results if \"faithfulness\" in r.metric_name.lower()), None\n",
    ")\n",
    "a_result = next(\n",
    "    (\n",
    "        r\n",
    "        for r in report_basic.results\n",
    "        if \"relevancy\" in r.metric_name.lower() or \"relevance\" in r.metric_name.lower()\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "f_scores: list[float] = (f_result.per_sample_scores if f_result else None) or []\n",
    "a_scores: list[float] = (a_result.per_sample_scores if a_result else None) or []\n",
    "\n",
    "\n",
    "def fmt(v: float) -> str:\n",
    "    return \"  N/A\" if math.isnan(v) else f\"{v:>5.2f}\"\n",
    "\n",
    "\n",
    "print(\"Per-sample scores (F = Faithfulness, A = AnswerRelevancy)\\n\")\n",
    "print(f\"{'#':<3} {'F':>4} {'A':>5} {'query':>7} {'response':>45}\")\n",
    "print(\"─\" * 100)\n",
    "for i, (sample, f, a) in enumerate(zip(samples, f_scores, a_scores), 1):\n",
    "    q = sample.query[:40] + \"..\" if len(sample.query) > 40 else sample.query\n",
    "    r = (\n",
    "        (sample.answer[:40] or \"\") + \"..\"\n",
    "        if len(sample.answer or \"\") > 60\n",
    "        else (sample.answer or \"\")\n",
    "    )\n",
    "    print(f\"{i:<3} {fmt(f)} {fmt(a)} {q:<40} {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9wjyrtsne7v",
   "metadata": {},
   "source": "---\n\n### Part 2: Context Retrieval Evaluation (Requires a Test Set)\n\nFaithfulness and AnswerRelevancy tell you about generation quality, but they say nothing about whether the retriever is finding the *right* chunks. For that you need a **test set**.\n\n#### What is a test set?\n\nA test set is a curated list of `(query, reference_answer)` pairs written by a domain expert. The reference answer serves as a proxy for \"what facts should be in the retrieved context\" — RAGAS uses it to judge retrieval without requiring you to manually label which chunks are relevant.\n\n**This project's test set** lives in `feature1_evaluation.py` → `EVALUATION_QUERIES`. It covers 7 domain scenarios: portfolio scope, claim verification, missing data, source conflicts, and policy queries.\n\n#### Context retrieval metrics\n\n| Metric | What it measures | Score = 1.0 means | Low score means |\n|---|---|---|---|\n| **ContextPrecision** | Are retrieved chunks relevant to the reference answer? Weighted by rank: relevant chunks ranked first score better. | Every retrieved chunk is relevant, and the most relevant are ranked first | The retriever returns noisy, off-topic chunks — or ranks them poorly |\n| **ContextRecall** | What fraction of the reference answer's claims can be attributed to the retrieved context? | Every fact needed to answer the question is present in the retrieved context | The retriever is missing chunks that contain key facts |\n\n**Interpretation:**\n- Low `ContextPrecision` → improve chunk filtering or reduce `top_k`\n- Low `ContextRecall` → relevant documents are not indexed, chunks are too small, or embedding mismatch\n- Both low → consider better chunking strategy or a different embedding model (see Feature Track 3)\n\n*Takes ~3–5 minutes: each metric makes multiple LLM calls per sample.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ubcdsjgifh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature1_evaluation import EVALUATION_QUERIES\n",
    "\n",
    "gt_queries = [q[\"query\"] for q in EVALUATION_QUERIES]\n",
    "gt_answers = [q[\"ground_truth_answer\"] for q in EVALUATION_QUERIES]\n",
    "\n",
    "print(f\"Test set: {len(gt_queries)} query-answer pairs\\n\")\n",
    "for i, (q, a) in enumerate(zip(gt_queries, gt_answers), 1):\n",
    "    print(f\"  {i}. {q}\")\n",
    "    print(f\"     → {a[:80]}{'...' if len(a) > 80 else ''}\\n\")\n",
    "\n",
    "print(f\"Building {len(gt_queries)} samples (runs the RAG agent once per query)...\")\n",
    "samples_gt = await Evaluator.build_samples_from_agent(\n",
    "    agent=agent,\n",
    "    queries=gt_queries,\n",
    "    ground_truth_answers=gt_answers,\n",
    ")\n",
    "print(f\"Done. {len(samples_gt)} samples built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bg3flddn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (  # type: ignore[attr-defined]\n",
    "    ContextPrecision as RagasContextPrecision,\n",
    "    ContextRecall as RagasContextRecall,\n",
    ")\n",
    "\n",
    "print(\"Running RAGAS: ContextPrecision + ContextRecall (~3-5 min)\\n\")\n",
    "\n",
    "report_context = evaluate_with_ragas(\n",
    "    samples=samples_gt,\n",
    "    metrics=[\n",
    "        RagasContextPrecision(),  # type: ignore[call-arg]\n",
    "        RagasContextRecall(),  # type: ignore[call-arg]\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"─\" * 40)\n",
    "print(f\"Samples evaluated: {report_context.num_samples}\")\n",
    "print(\"─\" * 40)\n",
    "for metric_name, score in report_context.summary().items():\n",
    "    print(f\"{metric_name:<28}  {score:.3f}\")\n",
    "print(\"─\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wkgfdwmhncd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown: find which queries have retrieval gaps\n",
    "cp_result = next(\n",
    "    (r for r in report_context.results if \"precision\" in r.metric_name.lower()), None\n",
    ")\n",
    "cr_result = next(\n",
    "    (r for r in report_context.results if \"recall\" in r.metric_name.lower()), None\n",
    ")\n",
    "\n",
    "cp_scores: list[float] = (cp_result.per_sample_scores if cp_result else None) or []\n",
    "cr_scores: list[float] = (cr_result.per_sample_scores if cr_result else None) or []\n",
    "\n",
    "print(\"Per-sample context scores (CP = ContextPrecision, CR = ContextRecall)\\n\")\n",
    "print(f\"{'#':<3} {'CP':>5} {'CR':>5}  {'query'}\")\n",
    "print(\"─\" * 85)\n",
    "for i, (sample, cp, cr) in enumerate(zip(samples_gt, cp_scores, cr_scores), 1):\n",
    "    q = sample.query[:68] + \"...\" if len(sample.query) > 68 else sample.query\n",
    "    print(f\"{i:<3} {fmt(cp):>5} {fmt(cr):>5}  {q}\")\n",
    "\n",
    "print()\n",
    "print(\n",
    "    \"A low CP score means the retriever returned off-topic chunks (or ranked them poorly).\"\n",
    ")\n",
    "print(\"A low CR score means key facts are missing from the retrieved context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f3a3b",
   "metadata": {},
   "source": "---\n\n## Summary and Next Steps\n\nYou now have four RAGAS metrics for the baseline pipeline:\n\n| Metric | Ground truth? | Tells you |\n|---|---|---|\n| `Faithfulness` | No | Are answers grounded in the retrieved context? |\n| `AnswerRelevancy` | No | Are answers on-topic? |\n| `ContextPrecision` | Yes | Are retrieved chunks relevant and well-ranked? |\n| `ContextRecall` | Yes | Does the retrieved context contain all necessary facts? |\n\n**How to use these scores:**\n- Run this notebook after every significant pipeline change (chunk size, `top_k`, embedding model, system prompt)\n- If `ContextPrecision` or `ContextRecall` are low, investigate the retrieval stage first — better generation cannot compensate for missing context\n- If `Faithfulness` is low despite good context retrieval, investigate the system prompt or model temperature\n- Add queries to `EVALUATION_QUERIES` in `feature1_evaluation.py` whenever you discover a new failure mode\n\n**Brainstorming & Tasks**"
  },
  {
   "cell_type": "markdown",
   "id": "f8be9f63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
