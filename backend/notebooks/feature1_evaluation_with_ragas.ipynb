{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p1-title-md",
   "metadata": {},
   "source": [
    "# Feature Track 1: Evaluation & Validation\n",
    "\n",
    "---\n",
    "\n",
    "Shipping a RAG system without systematic evaluation is like navigating without instruments. The pipeline may *seem* to work on the queries we tested by hand, but we have no way to know where it breaks, how often, or whether a change we made helped or hurt.\n",
    "\n",
    "**Evaluation closes the feedback loop:**\n",
    "\n",
    "```\n",
    "Change a parameter  ──►  Measure quantitatively  ──►  Decide based on data\n",
    "```\n",
    "\n",
    "**Prerequisite:** Run `feature0_baseline_rag.ipynb` Steps 1–2 first to build the vector store.\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| Feature 0 | Working baseline prototype |\n",
    "| **Feature Track 1 (this notebook)** | Quantitative evaluation |\n",
    "| Feature Track 2 | Reliable, structured outputs |\n",
    "| Feature Track 3 | Better retrieval strategies |\n",
    "| Feature Track 4 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-why-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Foundation\n",
    "\n",
    "### Why Systematic Evaluation?\n",
    "Without metrics, we are forced to manually re-read answers for a handful of test queries and guess whether a change helped or hurt. With metrics we run the evaluation suite and get a number -> one we can track across pipeline changes and use to justify decisions.\n",
    "\n",
    "**Concrete example from Feature 0:** The baseline RAG sometimes described a non-existent product as if it exists, cited a superseded GWP figure, or reported an unverified CO₂ reduction without flagging it. These are exactly the queries that matter for compliance. How often does this happen? After every change to the pipeline, we need an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-pipeline-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "Each arrow is a potential failure point. Evaluation targets a specific stage so we can isolate *where* a problem is.\n",
    "\n",
    "```\n",
    "Ingestion (run once)\n",
    "  Documents ──► [1] Chunker ──► [2] Embedder ──► [3] Vector DB\n",
    "\n",
    "Querying (every user question)\n",
    "  User query ──► [2] Embedder ──► [3] Retriever ──► Top-k Chunks\n",
    "                                                          │\n",
    "                                                   [4] LLM + Prompt\n",
    "                                                          │\n",
    "                                                   Answer + Sources\n",
    "```\n",
    "\n",
    "| Step | If it fails |\n",
    "|---|---|\n",
    "| [1] Chunking | Context split mid-fact; tables broken |\n",
    "| [2] Embedding | Wrong chunks returned |\n",
    "| [3] Vector search | Relevant chunks not retrieved |\n",
    "| [4] Generation | Hallucination; ignores context; incomplete answer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-ragas-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAGAS\n",
    "\n",
    "[RAGAS](https://docs.ragas.io) (*Retrieval Augmented Generation Assessment*) is an open-source Python library for evaluating RAG pipelines, widely adopted in the LLM/RAG ecosystem.\n",
    "\n",
    "#### How it works internally\n",
    "Rather than asking a judge LLM \"rate this answer 0–5\", RAGAS decomposes the answer into individual atomic claims:\n",
    "\n",
    "```\n",
    "Answer: \"The Logypal 1 GWP is 3.2 kg CO₂e, verified by Bureau Veritas.\"\n",
    "\n",
    "  Claim 1: \"GWP is 3.2 kg CO₂e\"          -> supported by context?  ✓\n",
    "  Claim 2: \"verified by Bureau Veritas\"  -> supported by context?  ✓\n",
    "\n",
    "  Faithfulness = 2 supported / 2 total = 1.0\n",
    "```\n",
    "\n",
    "This catches partial hallucination, e.g. a correct figure with a fabricated verifier name.\n",
    "\n",
    "#### Metrics overview\n",
    "\n",
    "| Metric | Ground truth needed? | What it catches |\n",
    "|---|---|---|\n",
    "| `Faithfulness` | No | Claims not supported by the retrieved context |\n",
    "| `AnswerRelevancy` | No | Off-topic or evasive answers |\n",
    "| `ContextPrecision` | **Yes** (reference answer) | Irrelevant chunks ranked above relevant ones |\n",
    "| `ContextRecall` | **Yes** (reference answer) | Facts needed to answer that are missing from the retrieved context |\n",
    "| `AnswerCorrectness` | **Yes** (reference answer) | Wrong or missing facts vs. the reference answer |\n",
    "\n",
    "The first two metrics can be run **on any query**, no labelling effort required. The last three require a **test set**: a curated list of (query, reference answer) pairs.\n",
    "\n",
    "#### Strengths and weaknesses\n",
    "- Standardised, reproducible metrics widely used in industry\n",
    "- `Faithfulness` and `AnswerRelevancy` require zero labelling effort\n",
    "- Needs a capable judge LLM (defaults to OpenAI), often costs multiple LLM calls per sample per metric\n",
    "- LLM judge has its own biases; metrics are proxies, not absolute ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### First Look at RAGAS\n",
    "\n",
    "#### Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` installed in editable mode. Vector store must already exist -> run `feature0_baseline_rag.ipynb` Steps 1–2 first.\n",
    "\n",
    "RAGAS uses OpenAI as its judge LLM by default ->`OPENAI_API_KEY` must be set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper  # type: ignore[import-untyped]\n",
    "from ragas.metrics import (  # type: ignore[attr-defined]\n",
    "    Faithfulness as RagasFaithfulness,\n",
    "    AnswerRelevancy as RagasAnswerRelevancy,\n",
    ")\n",
    "\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.evaluation import Evaluator\n",
    "from conversational_toolkit.evaluation.adapters import evaluate_with_ragas\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    EMBEDDING_MODEL,\n",
    "    VS_PATH,\n",
    "    SYSTEM_PROMPT,\n",
    "    build_llm,\n",
    "    build_agent,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "_secret_path = pathlib.Path(\"/secrets/OPENAI_API_KEY\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ and _secret_path.exists():\n",
    "    os.environ[\"OPENAI_API_KEY\"] = _secret_path.read_text().strip()\n",
    "\n",
    "RETRIEVER_TOP_K = 5\n",
    "BACKEND = \"openai\"  # \"ollama\" or \"openai\"\n",
    "# Note: RAGAS uses OpenAI for its judge LLM regardless of BACKEND above.\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError('Set BACKEND to \"ollama\" or \"openai\" before running.')\n",
    "\n",
    "# RAG pipeline\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n",
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vs,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    ")\n",
    "\n",
    "# AnswerRelevancy internally calls embed_query() / embed_documents() to compare generated questions against the original query. langchain_openai.OpenAIEmbeddings implements this interface and is accepted directly by ragas.evaluate().\n",
    "ragas_embeddings = LangChainOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# RAGAS defaults to max_tokens=3072 for its judge LLM. Long answers with many atomic claims overflow this limit mid-JSON, causing \"output is incomplete\" errors. Wrap ChatOpenAI with a higher limit and pass it explicitly to evaluate_with_ragas().\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=8192)\n",
    ")\n",
    "\n",
    "print(f\"Embedding model : {EMBEDDING_MODEL}\")\n",
    "print(f\"Vector store    : {VS_PATH}\")\n",
    "print(f\"RAG agent LLM   : {BACKEND}\")\n",
    "print(\"RAGAS judge LLM : gpt-4o-mini (OpenAI)\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d8fdd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 1: Metrics Without a Test Set\n",
    "\n",
    "These two metrics need only the query and the system's response, **no ground truth required**:\n",
    "\n",
    "- **Faithfulness**: are all claims in the answer supported by the retrieved context?\n",
    "- **AnswerRelevancy**: does the answer directly address the question?\n",
    "\n",
    "The `evaluate_with_ragas()` adapter converts our `EvaluationSample` objects to RAGAS format, runs the judge LLM, and returns an `EvaluationReport`.\n",
    "\n",
    "*Can take a few minutes: RAGAS makes multiple judge LLM calls per sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Does PrimePack AG offer a product called the Lara Pallet?\",\n",
    "    \"Which products in the portfolio have a third-party verified EPD?\",\n",
    "    \"Can the 68% CO2 reduction claim for tesapack ECO (product 50-102) be included in a customer sustainability response?\",\n",
    "    \"Are any tape products confirmed to be PFAS-free?\",\n",
    "    \"Which suppliers are not yet compliant with the EPD requirement by end of 2025?\",\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Building {len(queries)} evaluation samples (runs the RAG agent once per query)...\"\n",
    ")\n",
    "samples = await Evaluator.build_samples_from_agent(agent=agent, queries=queries)\n",
    "print(f\"Done. {len(samples)} samples built.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running RAGAS: Faithfulness + AnswerRelevancy\\n\")\n",
    "\n",
    "report_basic = evaluate_with_ragas(\n",
    "    samples=samples,\n",
    "    metrics=[\n",
    "        RagasFaithfulness(),  # type: ignore[call-arg]\n",
    "        RagasAnswerRelevancy(strictness=1),  # type: ignore[call-arg]\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"─\" * 40)\n",
    "print(f\"Samples evaluated: {report_basic.num_samples}\")\n",
    "print(\"─\" * 40)\n",
    "for metric_name, score in report_basic.summary().items():\n",
    "    print(f\"{metric_name:<22}  {score:.3f}\")\n",
    "print(\"─\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown: find which queries score best / worst so we know where to focus improvement efforts.\n",
    "\n",
    "import math\n",
    "\n",
    "f_result = next(\n",
    "    (r for r in report_basic.results if \"faithfulness\" in r.metric_name.lower()), None\n",
    ")\n",
    "a_result = next(\n",
    "    (\n",
    "        r\n",
    "        for r in report_basic.results\n",
    "        if \"relevancy\" in r.metric_name.lower() or \"relevance\" in r.metric_name.lower()\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "f_scores: list[float] = (f_result.per_sample_scores if f_result else None) or []\n",
    "a_scores: list[float] = (a_result.per_sample_scores if a_result else None) or []\n",
    "\n",
    "\n",
    "def fmt(v: float) -> str:\n",
    "    return \"  N/A\" if math.isnan(v) else f\"{v:>5.2f}\"\n",
    "\n",
    "\n",
    "print(\"Per-sample scores (F = Faithfulness, A = AnswerRelevancy)\\n\")\n",
    "print(f\"{'#':<3} {'F':>4} {'A':>5} {'query':>7} {'response':>45}\")\n",
    "print(\"─\" * 100)\n",
    "for i, (sample, f, a) in enumerate(zip(samples, f_scores, a_scores), 1):\n",
    "    q = sample.query[:40] + \"..\" if len(sample.query) > 40 else sample.query\n",
    "    r = (\n",
    "        (sample.answer[:40] or \"\") + \"..\"\n",
    "        if len(sample.answer or \"\") > 60\n",
    "        else (sample.answer or \"\")\n",
    "    )\n",
    "    print(f\"{i:<3} {fmt(f)} {fmt(a)} {q:<40} {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9wjyrtsne7v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Part 2: Context Retrieval Evaluation (Requires a Test Set)\n",
    "\n",
    "Faithfulness and AnswerRelevancy tell us about generation quality, but they say nothing about whether the retriever is finding the *right* chunks. For that we need a **test set**.\n",
    "\n",
    "#### What is a test set?\n",
    "\n",
    "A test set is a curated list of `(query, reference_answer)` pairs. The reference answer serves as a proxy for \"what facts should be in the retrieved context\" -> RAGAS uses it to judge retrieval without requiring us to manually label which chunks are relevant.\n",
    "\n",
    "#### Context retrieval metrics\n",
    "\n",
    "| Metric | What it measures | Score = 1.0 means | Low score means |\n",
    "|---|---|---|---|\n",
    "| **ContextPrecision** | Are retrieved chunks relevant to the reference answer? Weighted by rank: relevant chunks ranked first score better. | Every retrieved chunk is relevant, and the most relevant are ranked first | The retriever returns noisy, off-topic chunks or ranks them poorly |\n",
    "| **ContextRecall** | What fraction of the reference answer's claims can be attributed to the retrieved context? | Every fact needed to answer the question is present in the retrieved context | The retriever is missing chunks that contain key facts |\n",
    "\n",
    "**Interpretation:**\n",
    "- Low `ContextPrecision` -> improve chunk filtering or reduce `top_k`\n",
    "- Low `ContextRecall` -> relevant documents are not indexed, chunks are too small, or embedding mismatch\n",
    "- Both low -> consider better chunking strategy or a different embedding model (see Feature Track 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ac745",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_QUERIES: list[dict] = [\n",
    "    # portfolio scope\n",
    "    {\n",
    "        \"query\": \"Does PrimePack AG offer a product called the Lara Pallet?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"No. The Lara Pallet does not exist in the PrimePack AG portfolio. The current pallet portfolio is: Noé Pallet (32-100, CPR System), Wooden Pallet 1208 (32-101, CPR System), Recycled Plastic Pallet (32-102, CPR System), Logypal 1 (32-103, Relicyc), LogyLight (32-104, Relicyc), and EP 08 (32-105, StabilPlastik).\"\n",
    "        ),\n",
    "    },\n",
    "    # multi-product retrieval\n",
    "    {\n",
    "        \"query\": \"Which products in the portfolio have a third-party verified EPD?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"Products with third-party verified EPDs: 50-100 (IPG Hot Melt Tape), 50-101 (IPG Water-Activated Tape), 32-100 (Noé Pallet, CPR System), 32-103 (Logypal 1, Relicyc), 32-105 (EP 08, StabilPlastik), 11-100 (Cartonpallet CMP, redbox), 11-101 (Corrugated cardboard, Grupak).\"\n",
    "        ),\n",
    "    },\n",
    "    # claim verification\n",
    "    {\n",
    "        \"query\": \"Can the 68% CO2 reduction claim for tesapack ECO (product 50-102) be included in a customer sustainability response?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"No. The 68% CO2 reduction figure is a self-declared internal assessment by Tesa SE, not independently verified through an EPD. PrimePack AGs procurement policy classifies this as Level B/C evidence. It may only be cited with an explicit caveat that it is unverified.\"\n",
    "        ),\n",
    "    },\n",
    "    # missing data\n",
    "    {\n",
    "        \"query\": \"What verified environmental data is available for the LogyLight pallet (product 32-104)?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"No verified environmental data is available for LogyLight (32-104). The datasheet explicitly states that GWP and all other LCA figures are not yet available. An LCA study has been commissioned (REL-LCA-2024-07) and a third-party verified EPD was expected by Q2 2025, but no verified figures exist.\"\n",
    "        ),\n",
    "    },\n",
    "    # source conflict\n",
    "    {\n",
    "        \"query\": \"Which GWP source should be used for Relicyc Logypal 1: the 2021 datasheet or the 2023 EPD?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"The 2023 third-party verified EPD (Relicyc EPD No. S-P-10482) is the authoritative source. The 2021 internal datasheet reporting 4.1 kg CO2e per pallet is marked SUPERSEDED and must not be cited. When two sources conflict, PrimePack AGs policy requires preferring the more recent third-party verified source.\"\n",
    "        ),\n",
    "    },\n",
    "    # missing data\n",
    "    {\n",
    "        \"query\": \"Are any tape products confirmed to be PFAS-free?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"No tape product is confirmed PFAS-free. As of January 2025, no PFAS declarations have been received from IPG or Tesa SE. The Tesa hot-melt, free of intentionally added solvents claim does not constitute a PFAS declaration. No tape product may be described as PFAS-free until explicit supplier declarations are received and reviewed.\"\n",
    "        ),\n",
    "    },\n",
    "    # policy (tests procurement policy retrieval)\n",
    "    {\n",
    "        \"query\": \"Which suppliers are not yet compliant with the EPD requirement by end of 2025?\",\n",
    "        \"ground_truth_answer\": (\n",
    "            \"Tesa SE (supplier of tesapack ECO, product 50-102) and CPR System (supplier of Wooden Pallet 32-101 and Recycled Plastic Pallet 32-102) are not yet compliant with the EPD requirement by end of 2025.\"\n",
    "        ),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ubcdsjgifh",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_queries = [q[\"query\"] for q in EVALUATION_QUERIES]\n",
    "gt_answers = [q[\"ground_truth_answer\"] for q in EVALUATION_QUERIES]\n",
    "\n",
    "print(f\"Test set: {len(gt_queries)} query-answer pairs\\n\")\n",
    "for i, (q, a) in enumerate(zip(gt_queries, gt_answers), 1):\n",
    "    print(f\"  {i}. {q}\")\n",
    "    print(f\"     -> {a[:80]}{'...' if len(a) > 80 else ''}\\n\")\n",
    "\n",
    "print(f\"Building {len(gt_queries)} samples (runs the RAG agent once per query)...\")\n",
    "samples_gt = await Evaluator.build_samples_from_agent(\n",
    "    agent=agent,\n",
    "    queries=gt_queries,\n",
    "    ground_truth_answers=gt_answers,\n",
    ")\n",
    "print(f\"Done. {len(samples_gt)} samples built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bg3flddn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (  # type: ignore[attr-defined]\n",
    "    ContextPrecision as RagasContextPrecision,\n",
    "    ContextRecall as RagasContextRecall,\n",
    ")\n",
    "\n",
    "print(\"Running RAGAS: ContextPrecision + ContextRecall\\n\")\n",
    "\n",
    "report_context = evaluate_with_ragas(\n",
    "    samples=samples_gt,\n",
    "    metrics=[\n",
    "        RagasContextPrecision(),  # type: ignore[call-arg]\n",
    "        RagasContextRecall(),  # type: ignore[call-arg]\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"─\" * 40)\n",
    "print(f\"Samples evaluated: {report_context.num_samples}\")\n",
    "print(\"─\" * 40)\n",
    "for metric_name, score in report_context.summary().items():\n",
    "    print(f\"{metric_name:<28}  {score:.3f}\")\n",
    "print(\"─\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d687b4e",
   "metadata": {},
   "source": [
    "- A low `context_precision` score means the retriever returned off-topic chunks (or ranked them poorly).\n",
    "- A low `context_recall` score means key facts are missing from the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wkgfdwmhncd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown: find which queries have retrieval gaps\n",
    "cp_result = next(\n",
    "    (r for r in report_context.results if \"precision\" in r.metric_name.lower()), None\n",
    ")\n",
    "cr_result = next(\n",
    "    (r for r in report_context.results if \"recall\" in r.metric_name.lower()), None\n",
    ")\n",
    "\n",
    "cp_scores: list[float] = (cp_result.per_sample_scores if cp_result else None) or []\n",
    "cr_scores: list[float] = (cr_result.per_sample_scores if cr_result else None) or []\n",
    "\n",
    "print(\"Per-sample context scores (CP = ContextPrecision, CR = ContextRecall)\\n\")\n",
    "print(f\"{'#':<3} {'CP':>5} {'CR':>5}  {'query'}\")\n",
    "print(\"─\" * 85)\n",
    "for i, (sample, cp, cr) in enumerate(zip(samples_gt, cp_scores, cr_scores), 1):\n",
    "    q = sample.query[:68] + \"...\" if len(sample.query) > 68 else sample.query\n",
    "    print(f\"{i:<3} {fmt(cp):>5} {fmt(cr):>5}  {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f3a3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Qw now have four RAGAS metrics for the baseline pipeline:\n",
    "\n",
    "| Metric | Ground truth? | Tells you |\n",
    "|---|---|---|\n",
    "| `Faithfulness` | No | Are answers grounded in the retrieved context? |\n",
    "| `AnswerRelevancy` | No | Are answers on-topic? |\n",
    "| `ContextPrecision` | Yes | Are retrieved chunks relevant and well-ranked? |\n",
    "| `ContextRecall` | Yes | Does the retrieved context contain all necessary facts? |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
