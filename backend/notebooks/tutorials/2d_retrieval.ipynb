{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40720a3",
   "metadata": {},
   "source": [
    "# 2.d Retrieval\n",
    "\n",
    "In this notebook you will see:\n",
    "- How to retrieve based on embeddings\n",
    "- How to retrieve based on metadata\n",
    "- How to retrieve based on all of them.\n",
    "\n",
    "We will use as a parser the `docling`m the `SpecificCharChunker` chunker and `ChromaDBVectorStore` store.\n",
    "\n",
    "Other retrieval strategies exist, that are not covered here by creating a new instance of vector store and changing a bit the logic:\n",
    "- Not using `top_k` (threshold, ...)\n",
    "- Include keyword score (BM25, ...)\n",
    "- Make use of the document architecture (hierarchical retrieval, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901e832",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from utils.specific_chunker import SpecificCharChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f79d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_docs = \"data/docs\"\n",
    "path_to_document = os.path.join(path_to_docs, \"alexnet_paper.pdf\")\n",
    "\n",
    "path_to_db = \"data/db\"\n",
    "path_to_vectorstore = os.path.join(path_to_db, \"example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5932e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-26 15:19:53,964 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:53,977 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:53,977 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,066 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,068 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,068 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,117 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,127 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:19:54,127 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-02-26 15:20:05.418 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-26 15:20:06.212 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (96, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_converter = DocumentConverter()\n",
    "\n",
    "conv_res = doc_converter.convert(path_to_document)\n",
    "md = conv_res.document.export_to_markdown()\n",
    "\n",
    "# replace \\n per \" \", as often just new lines\n",
    "md = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", md)\n",
    "\n",
    "doc_title_to_document = {\"alexnet_paper.pdf\": md}\n",
    "\n",
    "chunker = SpecificCharChunker()\n",
    "chunks = chunker.make_chunks(\n",
    "    split_characters=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\"],\n",
    "    document_to_text=doc_title_to_document,\n",
    "    max_number_of_characters=1024,\n",
    ")\n",
    "\n",
    "if os.path.exists(path_to_vectorstore):\n",
    "    shutil.rmtree(path_to_vectorstore)\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "embeddings = await embedding_model.get_embeddings([c.content for c in chunks])\n",
    "\n",
    "vector_store = ChromaDBVectorStore(path_to_vectorstore)\n",
    "\n",
    "await vector_store.insert_chunks(chunks=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db457a18",
   "metadata": {},
   "source": [
    "# Embedding Retrieval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd6b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:20:06.765 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"\n",
    "query_embedding = await embedding_model.get_embeddings([query])\n",
    "\n",
    "retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "    embedding=query_embedding,\n",
    "    # The 5 best will be retrieved\n",
    "    top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b9efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      " Metadata: {'title': '67', 'doc_title': 'alexnet_paper.pdf', 'mime_type': 'text/markdown'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 2:\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      " Metadata: {'doc_title': 'alexnet_paper.pdf', 'mime_type': 'text/markdown', 'title': '69'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 3:\n",
      "ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n",
      " Metadata: {'mime_type': 'text/markdown', 'doc_title': 'alexnet_paper.pdf', 'title': '15'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 4:\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      " Metadata: {'mime_type': 'text/markdown', 'title': '73', 'doc_title': 'alexnet_paper.pdf'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 5:\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      " Metadata: {'title': '79', 'mime_type': 'text/markdown', 'doc_title': 'alexnet_paper.pdf'} \n",
      " ------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(\n",
    "        f\"Chunk {i + 1}:\\n{chunk.content}\\n\",\n",
    "        f\"Metadata: {chunk.metadata}\",\n",
    "        \"\\n\",\n",
    "        \"-\" * 30,\n",
    "        \"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bebd0",
   "metadata": {},
   "source": [
    "# Metadata Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0426c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:20:07.034 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"\n",
    "query_embedding = await embedding_model.get_embeddings([query])\n",
    "\n",
    "considered_metadata_title = [str(i) for i in range(60, 80)]\n",
    "# Any kind of SQL-like filter can be applied\n",
    "filters = {\"title\": {\"$in\": considered_metadata_title}}\n",
    "\n",
    "retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "    embedding=query_embedding,\n",
    "    top_k=5,\n",
    "    # Only consider chunks with a title in the range of 70 to 90 (inclusive)\n",
    "    filters=filters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221ef299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      " Metadata: {'mime_type': 'text/markdown', 'title': '67', 'doc_title': 'alexnet_paper.pdf'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 2:\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      " Metadata: {'title': '69', 'mime_type': 'text/markdown', 'doc_title': 'alexnet_paper.pdf'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 3:\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      " Metadata: {'title': '73', 'doc_title': 'alexnet_paper.pdf', 'mime_type': 'text/markdown'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 4:\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      " Metadata: {'mime_type': 'text/markdown', 'doc_title': 'alexnet_paper.pdf', 'title': '79'} \n",
      " ------------------------------ \n",
      "\n",
      "Chunk 5:\n",
      "| Model          | Top-1 (val)   | Top-5 (val)   | Top-5 (test)   | |----------------|---------------|---------------|----------------| | SIFT + FVs [7] | -             | -             | 26.2%          | | 1 CNN          | 40.7%         | 18.2%         | -              | | 5 CNNs         | 38.1%         | 16.4%         | 16.4%          | | 1 CNN*         | 39.0%         | 16.6%         | -              | | 7 CNNs*        | 36.7%         | 15.4%         | 15.3%          |\n",
      " Metadata: {'doc_title': 'alexnet_paper.pdf', 'mime_type': 'text/markdown', 'title': '74'} \n",
      " ------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(\n",
    "        f\"Chunk {i + 1}:\\n{chunk.content}\\n\",\n",
    "        f\"Metadata: {chunk.metadata}\",\n",
    "        \"\\n\",\n",
    "        \"-\" * 30,\n",
    "        \"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a69695",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
