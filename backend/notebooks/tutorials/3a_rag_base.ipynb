{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7304594",
   "metadata": {},
   "source": [
    "# 3.a Basic RAG\n",
    "\n",
    "In this notebook you will see:\n",
    "- How to create a basic RAG workflow\n",
    "- How to add naive query expansion\n",
    "\n",
    "Note that here a lot of improvements can be imagined, that depend on the use case (metadata usage, query contextualization, ...).\n",
    "\n",
    "What we did in previous sections is reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3197f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3177c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "from conversational_toolkit.llms.openai import OpenAILLM\n",
    "from conversational_toolkit.chunking.base import Chunk\n",
    "\n",
    "from utils.specific_chunker import SpecificCharChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ae016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_docs = \"data/docs\"\n",
    "path_to_document = os.path.join(path_to_docs, \"alexnet_paper.pdf\")\n",
    "\n",
    "path_to_db = \"data/db\"\n",
    "path_to_vectorstore = os.path.join(path_to_db, \"example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2985c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-26 15:23:50,242 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,249 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,249 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,330 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,332 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,333 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,379 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,384 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:23:50,384 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-02-26 15:24:01.515 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-26 15:24:02.374 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (96, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_converter = DocumentConverter()\n",
    "\n",
    "conv_res = doc_converter.convert(path_to_document)\n",
    "md = conv_res.document.export_to_markdown()\n",
    "\n",
    "# replace \\n per \" \", as often just new lines\n",
    "md = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", md)\n",
    "\n",
    "doc_title_to_document = {\"alexnet_paper.pdf\": md}\n",
    "\n",
    "chunker = SpecificCharChunker()\n",
    "chunks = chunker.make_chunks(\n",
    "    split_characters=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\"],\n",
    "    document_to_text=doc_title_to_document,\n",
    "    max_number_of_characters=1024,\n",
    ")\n",
    "\n",
    "if os.path.exists(path_to_vectorstore):\n",
    "    shutil.rmtree(path_to_vectorstore)\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "embeddings = await embedding_model.get_embeddings([c.content for c in chunks])\n",
    "\n",
    "vector_store = ChromaDBVectorStore(path_to_vectorstore)\n",
    "\n",
    "await vector_store.insert_chunks(chunks=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf22d5",
   "metadata": {},
   "source": [
    "# Define the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642f4846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:02.900 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4.1-mini; temperature: 0.5; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "# We already have the embeddings model and the vector store\n",
    "\n",
    "# We need the LLM\n",
    "llm = OpenAILLM(model_name=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce890aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, a function is required to convert the chunks into texts to be used in the prompt template\n",
    "def chunks_to_text(chunks: list[Chunk]) -> str:\n",
    "    text = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text += (\n",
    "            f\"## Chunk {chunk.title}:\\n```\\n{chunk.content}\\n```\\n\" + \"-\" * 30 + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # Remove the last separator\n",
    "    text = text[:-4]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438813c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need a system prompt for the RAG agent\n",
    "system_prompt = \"\"\"You are a helpful assistant that answers questions using the provided document excerpts (chunks).\n",
    "\n",
    "At each step, you will receive several chunks that are relevant to the user's question. Your task is to produce the best possible answer using only the information contained in those chunks.\n",
    "\n",
    "Rules you must follow:\n",
    "- Use the chunks as your only source of truth. Do not rely on outside knowledge.\n",
    "- Use all relevant chunks when forming your answer. Do not ignore any provided information.\n",
    "- If the answer cannot be found in the chunks, clearly say that you do not know.\n",
    "- Keep your answer concise and focused, without unnecessary details.\n",
    "- Cite your sources from the provided chunks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21929450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then a prompt template for the RAG agent\n",
    "prompt_template = \"\"\"# User question:\\n{question}\\n\\n# Relevant chunks:\\n{chunks}\\n\\n# Your answer:\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629abc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User question:\n",
      "What is the main contribution of the paper?\n",
      "\n",
      "# Relevant chunks:\n",
      "## Chunk 1:\n",
      "```\n",
      "## ImageNet Classification with Deep Convolutional Neural Networks\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 2:\n",
      "```\n",
      "Alex Krizhevsky University of Toronto kriz@cs.utoronto.ca\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 3:\n",
      "```\n",
      "Ilya Sutskever University of Toronto ilya@cs.utoronto.ca\n",
      "```\n",
      "----------------------------\n",
      "\n",
      "# Your answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        question=\"What is the main contribution of the paper?\",\n",
    "        chunks=chunks_to_text(chunks[:3]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e181f4",
   "metadata": {},
   "source": [
    "# Merging all\n",
    "\n",
    "In practice, this will be abstracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0fdda50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:03.124 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Get the relevant chunks\n",
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"\n",
    "query_embedding = await embedding_model.get_embeddings([query])\n",
    "retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "    embedding=query_embedding, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e389384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare conversation for the LLM\n",
    "system_prompt_message = LLMMessage(content=system_prompt, role=Roles.SYSTEM)\n",
    "filled_template = prompt_template.format(\n",
    "    question=query, chunks=chunks_to_text(retrieved_chunks)\n",
    ")\n",
    "query_message = LLMMessage(content=filled_template, role=Roles.USER)\n",
    "\n",
    "conversation = [system_prompt_message, query_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "466ebd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:05.057 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWZX7GkIhpDo6hebBjYN43uFiGFn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The top-1 and top-5 test set error rates obtained on ILSVRC-2010 by the network are 37.5% and 17.0%, respectively. These results outperform the best performance achieved during the ILSVRC-2010 competition, which were 47.1% top-1 error and 28.2% top-5 error, as well as the best published results before this work, which were 45.7% top-1 error and 25.7% top-5 error [Chunk 67].', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1772115843, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_a391f2cee0', usage=CompletionUsage(completion_tokens=115, prompt_tokens=708, total_tokens=823, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:24:05.057 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=115, prompt_tokens=708, total_tokens=823, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "answer = await llm.generate(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6f7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-1 and top-5 test set error rates obtained on ILSVRC-2010 by the network are 37.5% and 17.0%, respectively. These results outperform the best performance achieved during the ILSVRC-2010 competition, which were 47.1% top-1 error and 28.2% top-5 error, as well as the best published results before this work, which were 45.7% top-1 error and 25.7% top-5 error [Chunk 67].\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dba013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "\n",
      "You are a helpful assistant that answers questions using the provided document excerpts (chunks).\n",
      "\n",
      "At each step, you will receive several chunks that are relevant to the user's question. Your task is to produce the best possible answer using only the information contained in those chunks.\n",
      "\n",
      "Rules you must follow:\n",
      "- Use the chunks as your only source of truth. Do not rely on outside knowledge.\n",
      "- Use all relevant chunks when forming your answer. Do not ignore any provided information.\n",
      "- If the answer cannot be found in the chunks, clearly say that you do not know.\n",
      "- Keep your answer concise and focused, without unnecessary details.\n",
      "- Cite your sources from the provided chunks. \n",
      "\n",
      "\n",
      "QUERY:\n",
      "\n",
      "# User question:\n",
      "What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\n",
      "\n",
      "# Relevant chunks:\n",
      "## Chunk 67:\n",
      "```\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 69:\n",
      "```\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 15:\n",
      "```\n",
      "ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 73:\n",
      "```\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 79:\n",
      "```\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      "```\n",
      "----------------------------\n",
      "\n",
      "# Your answer:\n",
      "\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The LLM received:\n",
    "print(\"SYSTEM PROMPT:\\n\")\n",
    "print(system_prompt_message.content, \"\\n\\n\")\n",
    "print(\"QUERY:\\n\")\n",
    "print(query_message.content, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4bd80",
   "metadata": {},
   "source": [
    "# Query expansion\n",
    "\n",
    "Asking the LLM to generate alternatives queries helps to search more broadly and improve sometimes the original query. The implementation below is a very simple version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53fdbc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:05.292 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4.1-mini; temperature: 0.5; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'json_schema', 'json_schema': {'name': 'AnswerSchema', 'schema': {'type': 'object', 'name': 'AnswerSchema', 'description': 'The schema for the output of the query expansion. It contains 5 variations of the original query.', 'properties': {'expanded_queries': {'type': 'array', 'description': 'List of the 5 expanded queries.', 'items': {'type': 'string'}}}, 'required': ['expanded_queries'], 'additionalProperties': False}}}\n"
     ]
    }
   ],
   "source": [
    "# The system prompt of the call to expand the query before retrieval\n",
    "query_expansion_system_prompt = \"\"\"You are a helpful assistant that expands user queries to be more effective for retrieving relevant information from a vector store. Your role is to take a user query and expand it by proposing several variations of the query that might be more effective for retrieval. \n",
    "\n",
    "Propose 5 variations of the query, ensuring that they are semantically similar but use different wording or focus on different aspects of the original query. The goal is to increase the chances of retrieving relevant information from the vector store. Here is an example:\n",
    "\"\"\"\n",
    "\n",
    "# The schema of the output of the query expansion\n",
    "output_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"name\": \"AnswerSchema\",\n",
    "    \"description\": \"The schema for the output of the query expansion. It contains 5 variations of the original query.\",\n",
    "    \"properties\": {\n",
    "        \"expanded_queries\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of the 5 expanded queries.\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"expanded_queries\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"AnswerSchema\",\n",
    "        \"schema\": output_schema,\n",
    "    },\n",
    "}\n",
    "\n",
    "# The prompt template for the query expansion\n",
    "query_expansion_prompt_template = f\"\"\"# User query:\\n{query}\\n\\n# Expanded queries:\\n\"\"\"\n",
    "\n",
    "# The LLM instance to use for the query expansion\n",
    "llm = OpenAILLM(model_name=\"gpt-4.1-mini\", response_format=response_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a9d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:07.138 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWZZXiCTCed6CXsNrHeAws1zBv93', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"expanded_queries\":[\"What are the top-1 and top-5 accuracy scores for the ILSVRC-2010 dataset?\",\"Can you provide the top-1 and top-5 classification results on ILSVRC-2010?\",\"What top-1 and top-5 performance metrics were achieved in the ILSVRC 2010 challenge?\",\"Show the best top-1 and top-5 scores reported for ILSVRC-2010.\",\"What are the highest top-1 and top-5 scores recorded on the ImageNet Large Scale Visual Recognition Challenge 2010?\"]}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1772115845, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_a391f2cee0', usage=CompletionUsage(completion_tokens=124, prompt_tokens=211, total_tokens=335, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:24:07.138 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=124, prompt_tokens=211, total_tokens=335, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['What are the top-1 and top-5 accuracy scores for the ILSVRC-2010 dataset?', 'Can you provide the top-1 and top-5 classification results on ILSVRC-2010?', 'What top-1 and top-5 performance metrics were achieved in the ILSVRC 2010 challenge?', 'Show the best top-1 and top-5 scores reported for ILSVRC-2010.', 'What are the highest top-1 and top-5 scores recorded on the ImageNet Large Scale Visual Recognition Challenge 2010?']\n"
     ]
    }
   ],
   "source": [
    "# The user query\n",
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"\n",
    "\n",
    "# Call the LLM to expand the query\n",
    "filled_template_query_expansion = query_expansion_prompt_template.format(query=query)\n",
    "query_expansion_message = LLMMessage(\n",
    "    content=filled_template_query_expansion, role=Roles.USER\n",
    ")\n",
    "system_prompt_query_expansion_message = LLMMessage(\n",
    "    content=query_expansion_system_prompt, role=Roles.SYSTEM\n",
    ")\n",
    "conversation_query_expansion = [\n",
    "    system_prompt_query_expansion_message,\n",
    "    query_expansion_message,\n",
    "]\n",
    "\n",
    "# Get the expanded queries\n",
    "query_expansion_response = await llm.generate(conversation_query_expansion)\n",
    "\n",
    "# Extract them from the response\n",
    "expanded_queries = json.loads(query_expansion_response.content)[\"expanded_queries\"]\n",
    "print(len(expanded_queries))\n",
    "print(expanded_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a7b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:07.304 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: What are the top-1 and top-5 accuracy scores for the ILSVRC-2010 dataset?\n",
      "Number of retrieved chunks: 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:08.089 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: Can you provide the top-1 and top-5 classification results on ILSVRC-2010?\n",
      "Number of retrieved chunks: 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:08.625 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: What top-1 and top-5 performance metrics were achieved in the ILSVRC 2010 challenge?\n",
      "Number of retrieved chunks: 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:08.840 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n",
      "2026-02-26 15:24:09.007 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: Show the best top-1 and top-5 scores reported for ILSVRC-2010.\n",
      "Number of retrieved chunks: 5\n",
      "--------------------------------------------------\n",
      "Expanded query: What are the highest top-1 and top-5 scores recorded on the ImageNet Large Scale Visual Recognition Challenge 2010?\n",
      "Number of retrieved chunks: 5\n",
      "--------------------------------------------------\n",
      "Total number of unique retrieved chunks: 10\n"
     ]
    }
   ],
   "source": [
    "# Now let's retrieve the chunks for each of the expanded queries and see how many relevant chunks we get for each of them\n",
    "retrieved_chunks: list[Chunk] = []\n",
    "\n",
    "for expanded_query in expanded_queries:\n",
    "    expanded_query_embedding = await embedding_model.get_embeddings([expanded_query])\n",
    "    chunks_for_expanded_query = await vector_store.get_chunks_by_embedding(\n",
    "        embedding=expanded_query_embedding, top_k=5\n",
    "    )\n",
    "    retrieved_chunks.extend(chunks_for_expanded_query)\n",
    "\n",
    "    print(f\"Expanded query: {expanded_query}\")\n",
    "    print(f\"Number of retrieved chunks: {len(chunks_for_expanded_query)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Put them all together and remove duplicates\n",
    "unique_retrieved_chunks = {chunk.id: chunk for chunk in retrieved_chunks}.values()\n",
    "print(f\"Total number of unique retrieved chunks: {len(unique_retrieved_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b77c4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can go back to the original prompt template and prepare the conversation for the LLM with the retrieved chunks from all the expanded queries\n",
    "# Prepare conversation for the LLM\n",
    "system_prompt_message = LLMMessage(content=system_prompt, role=Roles.SYSTEM)\n",
    "filled_template = prompt_template.format(\n",
    "    question=query, chunks=chunks_to_text(unique_retrieved_chunks)\n",
    ")\n",
    "query_message = LLMMessage(content=filled_template, role=Roles.USER)\n",
    "\n",
    "conversation = [system_prompt_message, query_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99657d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:24:11.925 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWZdObVO7A9gR96bS382w6ZBXi19', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"expanded_queries\":[\"What are the top-1 and top-5 error rates achieved on the ILSVRC-2010 test set?\",\"Top-1 and top-5 scores obtained on the ILSVRC-2010 dataset?\",\"Performance metrics for ILSVRC-2010: top-1 and top-5 error rates?\",\"Reported top-1 and top-5 error rates for the ILSVRC-2010 competition?\",\"What were the achieved top-1 and top-5 error percentages on ILSVRC-2010?\"]}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1772115849, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_a391f2cee0', usage=CompletionUsage(completion_tokens=119, prompt_tokens=1804, total_tokens=1923, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:24:11.925 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=119, prompt_tokens=1804, total_tokens=1923, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "answer = await llm.generate(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf68d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"expanded_queries\":[\"What are the top-1 and top-5 error rates achieved on the ILSVRC-2010 test set?\",\"Top-1 and top-5 scores obtained on the ILSVRC-2010 dataset?\",\"Performance metrics for ILSVRC-2010: top-1 and top-5 error rates?\",\"Reported top-1 and top-5 error rates for the ILSVRC-2010 competition?\",\"What were the achieved top-1 and top-5 error percentages on ILSVRC-2010?\"]}\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4605aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM PROMPT:\n",
      "\n",
      "You are a helpful assistant that answers questions using the provided document excerpts (chunks).\n",
      "\n",
      "At each step, you will receive several chunks that are relevant to the user's question. Your task is to produce the best possible answer using only the information contained in those chunks.\n",
      "\n",
      "Rules you must follow:\n",
      "- Use the chunks as your only source of truth. Do not rely on outside knowledge.\n",
      "- Use all relevant chunks when forming your answer. Do not ignore any provided information.\n",
      "- If the answer cannot be found in the chunks, clearly say that you do not know.\n",
      "- Keep your answer concise and focused, without unnecessary details.\n",
      "- Cite your sources from the provided chunks. \n",
      "\n",
      "\n",
      "QUERY:\n",
      "\n",
      "# User question:\n",
      "What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\n",
      "\n",
      "# Relevant chunks:\n",
      "## Chunk 67:\n",
      "```\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 15:\n",
      "```\n",
      "ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 69:\n",
      "```\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 73:\n",
      "```\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 68:\n",
      "```\n",
      "We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 79:\n",
      "```\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 72:\n",
      "```\n",
      "Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 5:\n",
      "```\n",
      "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called 'dropout' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 106:\n",
      "```\n",
      "- [1] R.M. Bell and Y. Koren. Lessons from the netflix prize challenge. ACMSIGKDDExplorations Newsletter , 9(2):75-79, 2007. - [2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010. - [3] L. Breiman. Random forests. Machine learning , 45(1):5-32, 2001. - [4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745 , 2012. - [5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183 , 2011. - [6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09 , 2009. - [7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012 , 2012. URL http://www.image-net.org/challenges/LSVRC/2012/ . - [8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few \n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 14:\n",
      "```\n",
      "ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon's Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.\n",
      "```\n",
      "----------------------------\n",
      "\n",
      "# Your answer:\n",
      "\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The LLM received:\n",
    "print(\"SYSTEM PROMPT:\\n\")\n",
    "print(system_prompt_message.content, \"\\n\\n\")\n",
    "print(\"QUERY:\\n\")\n",
    "print(query_message.content, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc4a1d",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
