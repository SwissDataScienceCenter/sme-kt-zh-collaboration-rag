{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45605fb",
   "metadata": {},
   "source": [
    "# 3.c RAG in an agent\n",
    "\n",
    "In this notebook you will see\n",
    "- How to give the RAG tool to an agent (not to redefine the interpretation of the tool call, interpretation and send back each time)\n",
    "\n",
    "Again, a lot of improvements could be imagined there.\n",
    "\n",
    "What we did in previous sections is reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3818d9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90472c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "from conversational_toolkit.tools.base import Tool\n",
    "from conversational_toolkit.agents.tool_agent import ToolAgent\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "from conversational_toolkit.llms.openai import OpenAILLM\n",
    "from conversational_toolkit.chunking.base import Chunk\n",
    "\n",
    "from utils.specific_chunker import SpecificCharChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c9e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_docs = \"data/docs\"\n",
    "path_to_document = os.path.join(path_to_docs, \"alexnet_paper.pdf\")\n",
    "\n",
    "path_to_db = \"data/db\"\n",
    "path_to_vectorstore = os.path.join(path_to_db, \"example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e8a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-26 15:21:31,565 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,570 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,575 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,659 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,661 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,661 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,705 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,715 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:21:31,715 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-02-26 15:21:43.106 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-26 15:21:45.334 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (96, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_converter = DocumentConverter()\n",
    "\n",
    "conv_res = doc_converter.convert(path_to_document)\n",
    "md = conv_res.document.export_to_markdown()\n",
    "\n",
    "# replace \\n per \" \", as often just new lines\n",
    "md = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", md)\n",
    "\n",
    "doc_title_to_document = {\"alexnet_paper.pdf\": md}\n",
    "\n",
    "chunker = SpecificCharChunker()\n",
    "chunks = chunker.make_chunks(\n",
    "    split_characters=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\"],\n",
    "    document_to_text=doc_title_to_document,\n",
    "    max_number_of_characters=1024,\n",
    ")\n",
    "\n",
    "if os.path.exists(path_to_vectorstore):\n",
    "    shutil.rmtree(path_to_vectorstore)\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "embeddings = await embedding_model.get_embeddings([c.content for c in chunks])\n",
    "vector_store = ChromaDBVectorStore(path_to_vectorstore)\n",
    "\n",
    "await vector_store.insert_chunks(chunks=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5429c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_text(chunks: list[Chunk]) -> str:\n",
    "    text = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text += (\n",
    "            f\"## Chunk {chunk.title}:\\n```\\n{chunk.content}\\n```\\n\" + \"-\" * 30 + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    text = text[:-4]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "class RetrieveRelevantChunks(Tool):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        parameters: dict[str, Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.parameters = parameters\n",
    "\n",
    "    async def call(self, args: dict[str, Any]) -> dict[str, Any]:\n",
    "        query = args.get(\"query\")\n",
    "        top_k = args.get(\"top_k\", 5)\n",
    "\n",
    "        if top_k > 10:\n",
    "            raise ValueError(\"top_k cannot be greater than 10.\")\n",
    "\n",
    "        query_embedding = await embedding_model.get_embeddings([query])\n",
    "        retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "            embedding=query_embedding, top_k=top_k\n",
    "        )\n",
    "\n",
    "        retrieved_chunks_as_text = chunks_to_text(retrieved_chunks)\n",
    "\n",
    "        return {\"result\": retrieved_chunks_as_text}\n",
    "\n",
    "\n",
    "alexnet_retriever_tool = RetrieveRelevantChunks(\n",
    "    name=\"retrieve_relevant_chunks\",\n",
    "    description=\"Retrieves the most relevant chunks from the AlexNet paper based on a query.\",\n",
    "    # What parameters it expects\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The query to retrieve relevant chunks for.\",\n",
    "            },\n",
    "            \"top_k\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The number of top relevant chunks to retrieve, maximum is 10.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7a640",
   "metadata": {},
   "source": [
    "# Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38fa983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:45.866 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.5; seed: 42; tools: [<__main__.RetrieveRelevantChunks object at 0x000002854BA7B500>]; tool_choice: auto; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "class RAGAgent(ToolAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "llm = OpenAILLM(tool_choice=\"auto\", tools=[alexnet_retriever_tool])\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"You are a helpful assistant, answer shortly. Use the tools only when they are relevant, but if you do so, trust the results from the tools and use them in your answer, cite them precisely if you use them.\"\n",
    "prompt_as_message = LLMMessage(content=prompt, role=Roles.SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99380d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(system_prompt=prompt, llm=llm, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513284",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae97ac",
   "metadata": {},
   "source": [
    "## First Simple Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1555a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [prompt_as_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8ef676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:48.075 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': 'A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data, such as images. CNNs use convolutional layers to automatically detect and learn features from the input data, reducing the need for manual feature extraction. They are particularly effective in image recognition, classification, and computer vision tasks due to their ability to capture spatial hierarchies in images. CNNs typically consist of convolutional layers, pooling layers, and fully connected layers.', 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a CNN?\"\n",
    "query_as_message = LLMMessage(content=query, role=Roles.USER)\n",
    "query_with_context = QueryWithContext(query=query, history=[])\n",
    "\n",
    "answer = await rag_agent.answer(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efd9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data, such as images. CNNs use convolutional layers to automatically detect and learn features from the input data, reducing the need for manual feature extraction. They are particularly effective in image recognition, classification, and computer vision tasks due to their ability to capture spatial hierarchies in images. CNNs typically consist of convolutional layers, pooling layers, and fully connected layers.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6077491",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation += [query_as_message, answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e574d0",
   "metadata": {},
   "source": [
    "## Follow Up about AlexNet\n",
    "\n",
    "And test remembers conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7921325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:51.268 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n",
      "2026-02-26 15:21:51.504 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n",
      "2026-02-26 15:21:55.161 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': '', 'tool_calls': [ToolCall(id='call_qO0qnLW4YVkyR3TgeoKNRO4Z', function=Function(name='retrieve_relevant_chunks', arguments='{\"query\": \"ILSVRC-2010 AlexNet top-1 top-5 scores\"}'), type='function'), ToolCall(id='call_OUcCBcYCuSl88H4jEQKX5Y9K', function=Function(name='retrieve_relevant_chunks', arguments='{\"query\": \"AlexNet CNN\"}'), type='function')], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}, {'result': \"## Chunk 15:\\n```\\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\\n```\\n------------------------------\\n\\n## Chunk 5:\\n```\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called 'dropout' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\n```\\n------------------------------\\n\\n## Chunk 67:\\n```\\nOur results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\\n```\\n------------------------------\\n\\n## Chunk 69:\\n```\\nTable 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\\n```\\n------------------------------\\n\\n## Chunk 73:\\n```\\nTable 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\\n```\\n----------------------------\", 'role': 'tool', 'function_name': 'retrieve_relevant_chunks'}, {'result': \"## Chunk 1:\\n```\\n## ImageNet Classification with Deep Convolutional Neural Networks\\n```\\n------------------------------\\n\\n## Chunk 5:\\n```\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called 'dropout' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\n```\\n------------------------------\\n\\n## Chunk 10:\\n```\\nDespite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.\\n```\\n------------------------------\\n\\n## Chunk 41:\\n```\\nFigure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network's input is 150,528-dimensional, and the number of neurons in the network's remaining layers is given by 253,440-186,624-64,896-64,896-43,2644096-4096-1000.\\n```\\n------------------------------\\n\\n## Chunk 106:\\n```\\n- [1] R.M. Bell and Y. Koren. Lessons from the netflix prize challenge. ACMSIGKDDExplorations Newsletter , 9(2):75-79, 2007. - [2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010. - [3] L. Breiman. Random forests. Machine learning , 45(1):5-32, 2001. - [4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745 , 2012. - [5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183 , 2011. - [6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09 , 2009. - [7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012 , 2012. URL http://www.image-net.org/challenges/LSVRC/2012/ . - [8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few \\n```\\n----------------------------\", 'role': 'tool', 'function_name': 'retrieve_relevant_chunks'}, {'content': 'AlexNet achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0% on the ILSVRC-2010 dataset, significantly outperforming previous models at the time. This performance was a result of its deep convolutional architecture, which includes five convolutional layers and three fully connected layers, leveraging techniques like dropout for regularization and efficient GPU implementations for training.\\n\\nAlexNet is a specific type of Convolutional Neural Network (CNN), designed for image classification tasks, which explains its relevance to your first question about CNNs. Its architecture and training methods have had a substantial impact on the field of deep learning and computer vision.', 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010' by AlexNet? How does AlexNet relate to my first question? Answer to both in few sentences.\"\n",
    "query_as_message = LLMMessage(content=query, role=Roles.USER)\n",
    "query_with_context = QueryWithContext(query=query, history=conversation)\n",
    "\n",
    "answer = await rag_agent.answer(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aca9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0% on the ILSVRC-2010 dataset, significantly outperforming previous models at the time. This performance was a result of its deep convolutional architecture, which includes five convolutional layers and three fully connected layers, leveraging techniques like dropout for regularization and efficient GPU implementations for training.\n",
      "\n",
      "AlexNet is a specific type of Convolutional Neural Network (CNN), designed for image classification tasks, which explains its relevance to your first question about CNNs. Its architecture and training methods have had a substantial impact on the field of deep learning and computer vision.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7345f",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
