{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae793c61",
   "metadata": {},
   "source": [
    "# 3.b RAG as tool\n",
    "\n",
    "In this notebook you will see:\n",
    "- How to implement RAG as a tool\n",
    "- See how the LLM might (or not) use the tool w.r.t. the query.\n",
    "\n",
    "Again, a lot of improvements could be imagined there.\n",
    "\n",
    "What we did in previous sections is reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72123cc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b6a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "from conversational_toolkit.tools.base import Tool\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "from conversational_toolkit.llms.openai import OpenAILLM\n",
    "from conversational_toolkit.chunking.base import Chunk\n",
    "\n",
    "from utils.specific_chunker import SpecificCharChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ffaa0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_docs = \"data/docs\"\n",
    "path_to_document = os.path.join(path_to_docs, \"alexnet_paper.pdf\")\n",
    "\n",
    "path_to_db = \"data/db\"\n",
    "path_to_vectorstore = os.path.join(path_to_db, \"example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d9510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-26 15:20:52,644 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,655 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,656 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,738 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,740 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,741 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,804 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,815 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:20:52,816 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-02-26 15:21:04.115 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-26 15:21:05.014 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (96, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_converter = DocumentConverter()\n",
    "\n",
    "conv_res = doc_converter.convert(path_to_document)\n",
    "md = conv_res.document.export_to_markdown()\n",
    "\n",
    "# replace \\n per \" \", as often just new lines\n",
    "md = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", md)\n",
    "\n",
    "doc_title_to_document = {\"alexnet_paper.pdf\": md}\n",
    "\n",
    "chunker = SpecificCharChunker()\n",
    "chunks = chunker.make_chunks(\n",
    "    split_characters=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\"],\n",
    "    document_to_text=doc_title_to_document,\n",
    "    max_number_of_characters=1024,\n",
    ")\n",
    "\n",
    "if os.path.exists(path_to_vectorstore):\n",
    "    shutil.rmtree(path_to_vectorstore)\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "embeddings = await embedding_model.get_embeddings([c.content for c in chunks])\n",
    "vector_store = ChromaDBVectorStore(path_to_vectorstore)\n",
    "\n",
    "await vector_store.insert_chunks(chunks=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ee252",
   "metadata": {},
   "source": [
    "# Create the tool\n",
    "\n",
    "First, let's create a tool that sends back to the LLM the relevant chunks, if it is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a60a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_text(chunks: list[Chunk]) -> str:\n",
    "    text = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text += (\n",
    "            f\"## Chunk {chunk.title}:\\n```\\n{chunk.content}\\n```\\n\" + \"-\" * 30 + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    text = text[:-4]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2842456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveRelevantChunks(Tool):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        parameters: dict[str, Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.parameters = parameters\n",
    "\n",
    "    async def call(self, args: dict[str, Any]) -> dict[str, Any]:\n",
    "        query = args.get(\"query\")\n",
    "        top_k = args.get(\"top_k\", 5)\n",
    "\n",
    "        if top_k > 10:\n",
    "            raise ValueError(\"top_k cannot be greater than 10.\")\n",
    "\n",
    "        query_embedding = await embedding_model.get_embeddings([query])\n",
    "        retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "            embedding=query_embedding, top_k=top_k\n",
    "        )\n",
    "\n",
    "        retrieved_chunks_as_text = chunks_to_text(retrieved_chunks)\n",
    "\n",
    "        return {\"result\": retrieved_chunks_as_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377c06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_retriever_tool = RetrieveRelevantChunks(\n",
    "    name=\"retrieve_relevant_chunks\",\n",
    "    description=\"Retrieves the most relevant chunks from the AlexNet paper based on a query.\",\n",
    "    # What parameters it expects\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The query to retrieve relevant chunks for.\",\n",
    "            },\n",
    "            \"top_k\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The number of top relevant chunks to retrieve, maximum is 10.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5628a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:05.516 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chunk 67:\n",
      "```\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      "```\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "result = await alexnet_retriever_tool.call(\n",
    "    {\n",
    "        \"query\": \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\",\n",
    "        \"top_k\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca15091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:05.834 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chunk 67:\n",
      "```\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 69:\n",
      "```\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 15:\n",
      "```\n",
      "ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 73:\n",
      "```\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 79:\n",
      "```\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      "```\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test if it works without top_k, should default to 5\n",
    "result = await alexnet_retriever_tool.call(\n",
    "    {\"query\": \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"}\n",
    ")\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65ac65",
   "metadata": {},
   "source": [
    "# Provide the tool to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5e9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant that answers questions.\n",
    "\n",
    "You have access to the following tool:\n",
    "- retrieve_relevant_chunks: Retrieves the most relevant chunks from the AlexNet paper based on a query.\n",
    "\n",
    "Only use the tool if it's relevant, else answer based on your own knowledge. Always try to use the tool if you think it can help you answer the question better.\n",
    "\n",
    "If you use the tool, follow these guidelines:\n",
    "- Use the chunks as your only source of truth. Do not rely on outside knowledge.\n",
    "- Use all relevant chunks when forming your answer. Do not ignore any provided information.\n",
    "- If the answer cannot be found in the chunks, clearly say that you do not know.\n",
    "- Keep your answer concise and focused, without unnecessary details.\n",
    "- Cite your sources from the provided chunks.\"\"\"\n",
    "\n",
    "prompt_message = LLMMessage(content=system_prompt, role=Roles.SYSTEM)\n",
    "\n",
    "prompt_template = \"\"\"# User question:\\n{question}\\n\\nYour answer:\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87dc26fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:06.116 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.5; seed: 42; tools: [<__main__.RetrieveRelevantChunks object at 0x0000023604E80B30>]; tool_choice: auto; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAILLM(tools=[alexnet_retriever_tool], tool_choice=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb13d8c",
   "metadata": {},
   "source": [
    "# Test the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c9b83",
   "metadata": {},
   "source": [
    "## General Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15d3d076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:08.570 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWWgBVzJaXA16SDTL9ct7BzIUhm6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Einstein's theory of relativity comprises two main parts: special relativity and general relativity. Special relativity, introduced in 1905, establishes that the laws of physics are the same for all non-accelerating observers and that the speed of light is constant, leading to the famous equation E=mc². General relativity, published in 1915, generalizes this concept to include gravity, describing it as the curvature of spacetime caused by mass.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1772115666, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_414ba99a04', usage=CompletionUsage(completion_tokens=96, prompt_tokens=259, total_tokens=355, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:21:08.571 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=96, prompt_tokens=259, total_tokens=355, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Einstein's theory of relativity? Answer concisely in 2-3 sentences.\"\n",
    "user_message = LLMMessage(role=Roles.USER, content=query)\n",
    "\n",
    "response = await llm.generate(conversation=[prompt_message, user_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834a4f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMMessage(content=\"Einstein's theory of relativity comprises two main parts: special relativity and general relativity. Special relativity, introduced in 1905, establishes that the laws of physics are the same for all non-accelerating observers and that the speed of light is constant, leading to the famous equation E=mc². General relativity, published in 1915, generalizes this concept to include gravity, describing it as the curvature of spacetime caused by mass.\", role=<Roles.ASSISTANT: 'assistant'>, tool_calls=[], tool_call_id=None, name=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the LLM should not call the tool, as it can answer based on its own knowledge\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f875d2",
   "metadata": {},
   "source": [
    "## AlexNet Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1d93900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:09.666 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWWiCeyqr5rcBZCBF9XHaE3Vibdq', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_crFhBPpXEa1zdrX0tGSX81uh', function=Function(arguments='{\"query\":\"top-1 and top-5 scores ILSVRC-2010\"}', name='retrieve_relevant_chunks'), type='function')]))], created=1772115668, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_b8d58b1866', usage=CompletionUsage(completion_tokens=30, prompt_tokens=260, total_tokens=290, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:21:09.667 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=30, prompt_tokens=260, total_tokens=290, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "# Let's ask a question about AlexNet\n",
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010'?\"\n",
    "user_message = LLMMessage(role=Roles.USER, content=query)\n",
    "\n",
    "response = await llm.generate(conversation=[prompt_message, user_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2517e3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMMessage(content='', role=<Roles.ASSISTANT: 'assistant'>, tool_calls=[ToolCall(id='call_crFhBPpXEa1zdrX0tGSX81uh', function=Function(name='retrieve_relevant_chunks', arguments='{\"query\":\"top-1 and top-5 scores ILSVRC-2010\"}'), type='function')], tool_call_id=None, name=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the LLM indeed calls the tool\n",
    "# As sown in 1c, now it should be forwarded\n",
    "# Note that the query is not the same, the LLM rewrote it\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d57811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:12.735 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chunk 69:\n",
      "```\n",
      "Table 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 67:\n",
      "```\n",
      "Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 15:\n",
      "```\n",
      "ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 73:\n",
      "```\n",
      "Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were 'pre-trained' to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\n",
      "```\n",
      "------------------------------\n",
      "\n",
      "## Chunk 79:\n",
      "```\n",
      "Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.\n",
      "```\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    tool_name = tool_call.function.name\n",
    "    tool_args = tool_call.function.arguments\n",
    "\n",
    "    tool = next((t for t in llm.tools if t.name == tool_name), None)\n",
    "\n",
    "    if tool is not None:\n",
    "        tools_args_json = json.loads(tool_args)\n",
    "        tool_result = await tool.call(tools_args_json)\n",
    "\n",
    "        results[tool_name] = tool_result\n",
    "\n",
    "print(results[\"retrieve_relevant_chunks\"][\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82fc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_answers = []\n",
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    tool_name = tool_call.function.name\n",
    "    result = results[tool_name]\n",
    "    call_id = tool_call.id\n",
    "\n",
    "    tool_answer = LLMMessage(\n",
    "        role=Roles.TOOL,\n",
    "        name=tool_name,\n",
    "        content=json.dumps(result),\n",
    "        tool_call_id=call_id,\n",
    "    )\n",
    "    tools_answers.append(tool_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bf130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:21:15.275 | DEBUG    | conversational_toolkit.llms.openai:generate:87 - Completion: ChatCompletion(id='chatcmpl-DDWWmWbmGpKVWhdYoQwfqZkCy3L3p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The top-1 and top-5 scores obtained on the ILSVRC-2010 are as follows:\\n\\n- **Top-1 error rate**: 37.5%\\n- **Top-5 error rate**: 17.0%\\n\\nThe best performance during the ILSVRC-2010 competition was 47.1% for top-1 and 28.2% for top-5.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1772115672, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_373a14eb6f', usage=CompletionUsage(completion_tokens=86, prompt_tokens=712, total_tokens=798, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2026-02-26 15:21:15.275 | INFO     | conversational_toolkit.llms.openai:generate:88 - LLM Usage: CompletionUsage(completion_tokens=86, prompt_tokens=712, total_tokens=798, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "conversation = [user_message, response, *tools_answers]\n",
    "\n",
    "final_response = await llm.generate(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e3a5910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-1 and top-5 scores obtained on the ILSVRC-2010 are as follows:\n",
      "\n",
      "- **Top-1 error rate**: 37.5%\n",
      "- **Top-5 error rate**: 17.0%\n",
      "\n",
      "The best performance during the ILSVRC-2010 competition was 47.1% for top-1 and 28.2% for top-5.\n"
     ]
    }
   ],
   "source": [
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638a688",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
