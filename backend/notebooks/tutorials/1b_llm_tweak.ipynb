{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91350f1f",
   "metadata": {},
   "source": [
    "# 1.b Tweak the LLM\n",
    "\n",
    "In this notebook you will see:\n",
    "- How to change the model parameters\n",
    "- How to ask the LLM for a specific output format\n",
    "- How to estimate the cost of a query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1828dc6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806535f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "from loguru import logger\n",
    "import tiktoken\n",
    "\n",
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "from conversational_toolkit.llms.openai import OpenAILLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1aeef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove logging\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"ERROR\", filter=lambda record: record[\"level\"].no < 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377d34a",
   "metadata": {},
   "source": [
    "# Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5effb",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Different models can be used, they will vary in cost, efficiency, modality, ... This is controlled by the `model_name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b793ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user message (answer should be 12)\n",
    "query = \"What is (3^3-3)/2? Be short.\"\n",
    "user_message = LLMMessage(role=Roles.USER, content=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc90d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\((3^3 - 3)/2 = (27 - 3)/2 = 24/2 = 12\\)\n"
     ]
    }
   ],
   "source": [
    "# Define and call the LLM with GPT-4.1-mini model\n",
    "llm_gpt_4_1_mini = OpenAILLM(model_name=\"gpt-4.1-mini\")\n",
    "llm_gpt_4_1_mini_response = await llm_gpt_4_1_mini.generate([user_message])\n",
    "\n",
    "print(llm_gpt_4_1_mini_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18baac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5\n"
     ]
    }
   ],
   "source": [
    "# Define and call the LLM with GPT-3.5-turbo model, now it's wrong\n",
    "llm_gpt_3_5_turbo = OpenAILLM(model_name=\"gpt-3.5-turbo\")\n",
    "llm_gpt_3_5_turbo_response = await llm_gpt_3_5_turbo.generate([user_message])\n",
    "\n",
    "print(llm_gpt_3_5_turbo_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e571c",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature influences the randomness/creativity of the model output. This is controlled by the `temperature` parameter. \n",
    "To test this, we iterate to see different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cda82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the color of the sky that pirates prefer? Be short (only 3 words).\"\n",
    "user_message = LLMMessage(role=Roles.USER, content=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b1fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1, Iteration: 1, Response: Black as night\n",
      "Temperature: 0.1, Iteration: 2, Response: Black as night\n",
      "Temperature: 0.1, Iteration: 3, Response: Black as night\n",
      "-----\n",
      "Temperature: 0.3, Iteration: 1, Response: Black as night\n",
      "Temperature: 0.3, Iteration: 2, Response: Black as night\n",
      "Temperature: 0.3, Iteration: 3, Response: Black as night\n",
      "-----\n",
      "Temperature: 0.8, Iteration: 1, Response: Clear and blue\n",
      "Temperature: 0.8, Iteration: 2, Response: Clear and blue\n",
      "Temperature: 0.8, Iteration: 3, Response: Clear blue skies\n",
      "-----\n",
      "Temperature: 1.0, Iteration: 1, Response: Clear and blue\n",
      "Temperature: 1.0, Iteration: 2, Response: Clear blue skies\n",
      "Temperature: 1.0, Iteration: 3, Response: Clear and blue\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for temperature in [0.1, 0.3, 0.8, 1.0]:\n",
    "    # Each time use the same LLM but with different temperature\n",
    "    for it in range(3):\n",
    "        llm = OpenAILLM(model_name=\"gpt-4.1-mini\", temperature=temperature)\n",
    "        response = await llm.generate([user_message])\n",
    "        print(\n",
    "            f\"Temperature: {temperature}, Iteration: {it + 1}, Response: {response.content}\"\n",
    "        )\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c01745",
   "metadata": {},
   "source": [
    "# Structured Output\n",
    "\n",
    "It's possible to ask models to answer in a specific format, either by prompting them to do so, however this typically leads to unreliable results.\n",
    "\n",
    "To mitigate that, some LLMs allow to provide a schema in order to enforce the LLM to complete it as required, and thus automatize processes (see [link](https://docs.pydantic.dev/latest/)).\n",
    "\n",
    "For each properties, one can specify the type, if it mandatory, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248d57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I picked up 'The Clockwork Orchard' by Mira Ellison during my trip and couldn't stop reading it on the train.\",\n",
    "    \"My friend recommended 'A Map of Forgotten Rivers' by Daniel Cho and Lina Petrov for our book club this month.\",\n",
    "    \"At the old bookstore, I discovered 'Whispers in the Library' with no listed author, which made it even more mysterious.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6897ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"name\": \"AnswerSchema\",\n",
    "    # Describe what is this schema for\n",
    "    \"description\": \"The structured output for the user's answer\",\n",
    "    \"properties\": {\n",
    "        # Define a first property 'title'\n",
    "        \"title\": {\n",
    "            # It's a string\n",
    "            \"type\": \"string\",\n",
    "            # It should contain the title of the book\n",
    "            \"description\": \"Title of the book\",\n",
    "        },\n",
    "        # Define a second property 'authors'\n",
    "        \"authors\": {\n",
    "            # It's an array\n",
    "            \"type\": \"array\",\n",
    "            # It should contain list of authors\n",
    "            \"description\": \"List of the authors\",\n",
    "            \"items\": {\n",
    "                # Each item in the array is a string\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    # The 'title' property is required, others are optional\n",
    "    \"required\": [\"title\"],\n",
    "    # No additional properties are allowed\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"AnswerSchema\",\n",
    "        \"schema\": output_schema,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78329958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM and the query that will be asked for each sentence\n",
    "llm = OpenAILLM(response_format=response_format)\n",
    "query = \"Please extract the title and authors from this sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af03211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Clockwork Orchard\n",
      "Authors:  ['Mira Ellison']\n",
      "-----\n",
      "Title:  A Map of Forgotten Rivers\n",
      "Authors:  ['Daniel Cho', 'Lina Petrov']\n",
      "-----\n",
      "Title:  Whispers in the Library\n",
      "Authors:  []\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "answers_as_dict = []\n",
    "\n",
    "# Iterate over each sentence and get structured response from LLM\n",
    "for sentence in sentences:\n",
    "    user_message = LLMMessage(\n",
    "        role=Roles.USER, content=f\"{query}\\n\\nSentence: {sentence}\"\n",
    "    )\n",
    "    answer = await llm.generate([user_message])\n",
    "    answers_as_dict.append(json.loads(answer.content))\n",
    "\n",
    "for answer in answers_as_dict:\n",
    "    print(\"Title: \", answer.get(\"title\"))\n",
    "    print(\"Authors: \", answer.get(\"authors\"))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2abb1d",
   "metadata": {},
   "source": [
    "# Cost estimation\n",
    "\n",
    "Computing the number of tokens sent and generated are important, as they will define the latency and/or the cost of the tool. Note that the most costly/slower is usually the output tokens (from the LLM), not the one sent to the LLM (user query, prompt, history, ...). The computation depends on the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41eaf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price per token (USD) for gpt-4.1-mini\n",
    "INPUT_PRICE = 0.40 / 1_000_000\n",
    "OUTPUT_PRICE = 1.60 / 1_000_000\n",
    "\n",
    "# Function to count tokens using tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "def tokens(text):\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "# Function to calculate conversation cost\n",
    "def conversation_cost(messages):\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "\n",
    "    for m in messages:\n",
    "        n = tokens(m.content)\n",
    "        if m.role == \"assistant\":\n",
    "            output_tokens += n\n",
    "        else:\n",
    "            input_tokens += n\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_cost_usd\": input_tokens * INPUT_PRICE + output_tokens * OUTPUT_PRICE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6bee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Write a catchy slogan for a coffee shop run by robots.\n",
      "LLM:  \"Perk Up with Precision â€“ Brewed by Bots, Crafted for You!\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 12,\n",
       " 'output_tokens': 16,\n",
       " 'total_cost_usd': 3.0400000000000004e-05}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Write a catchy slogan for a coffee shop run by robots.\"\n",
    "user_message = LLMMessage(role=Roles.USER, content=query)\n",
    "\n",
    "llm = OpenAILLM(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "answer = await llm.generate([user_message])\n",
    "\n",
    "print(\"User: \", user_message.content)\n",
    "print(\"LLM: \", answer.content)\n",
    "\n",
    "conversation = [user_message, answer]\n",
    "\n",
    "conversation_cost(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ecf35",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
