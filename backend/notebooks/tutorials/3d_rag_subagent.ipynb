{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6138e388",
   "metadata": {},
   "source": [
    "# 3.d RAG as subagent\n",
    "\n",
    "In this notebook you will see\n",
    "- How to have the RAG as an agent called by another agent (avoid having to explain to the main agent how to interpret the sources)\n",
    "\n",
    "Again, a lot of improvements could be imagined there.\n",
    "\n",
    "What we did in previous sections is reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0ef14",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d1466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "from conversational_toolkit.tools.base import Tool\n",
    "from conversational_toolkit.agents.tool_agent import ToolAgent\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "from conversational_toolkit.llms.openai import OpenAILLM\n",
    "from conversational_toolkit.chunking.base import Chunk\n",
    "\n",
    "from utils.specific_chunker import SpecificCharChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b24818",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_docs = \"data/docs\"\n",
    "path_to_document = os.path.join(path_to_docs, \"alexnet_paper.pdf\")\n",
    "\n",
    "path_to_db = \"data/db\"\n",
    "path_to_vectorstore = os.path.join(path_to_db, \"example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb99906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-26 15:22:33,054 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,068 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,068 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,146 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,148 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,149 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,199 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,209 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-26 15:22:33,209 [RapidOCR] main.py:53: Using C:\\Users\\sieverin\\SDSC\\Code\\sme-kt-zh-collaboration-rag\\rag_venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-02-26 15:22:43.995 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-26 15:22:44.628 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (96, 1024)\n"
     ]
    }
   ],
   "source": [
    "doc_converter = DocumentConverter()\n",
    "\n",
    "conv_res = doc_converter.convert(path_to_document)\n",
    "md = conv_res.document.export_to_markdown()\n",
    "\n",
    "# replace \\n per \" \", as often just new lines\n",
    "md = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", md)\n",
    "\n",
    "doc_title_to_document = {\"alexnet_paper.pdf\": md}\n",
    "\n",
    "chunker = SpecificCharChunker()\n",
    "chunks = chunker.make_chunks(\n",
    "    split_characters=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\"],\n",
    "    document_to_text=doc_title_to_document,\n",
    "    max_number_of_characters=1024,\n",
    ")\n",
    "\n",
    "if os.path.exists(path_to_vectorstore):\n",
    "    shutil.rmtree(path_to_vectorstore)\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "embeddings = await embedding_model.get_embeddings([c.content for c in chunks])\n",
    "vector_store = ChromaDBVectorStore(path_to_vectorstore)\n",
    "\n",
    "await vector_store.insert_chunks(chunks=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310ee0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_text(chunks: list[Chunk]) -> str:\n",
    "    text = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text += (\n",
    "            f\"## Chunk {chunk.title}:\\n```\\n{chunk.content}\\n```\\n\" + \"-\" * 30 + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    text = text[:-4]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "class RetrieveRelevantChunks(Tool):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        parameters: dict[str, Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.parameters = parameters\n",
    "\n",
    "    async def call(self, args: dict[str, Any]) -> dict[str, Any]:\n",
    "        query = args.get(\"query\")\n",
    "        top_k = args.get(\"top_k\", 5)\n",
    "\n",
    "        if top_k > 10:\n",
    "            raise ValueError(\"top_k cannot be greater than 10.\")\n",
    "\n",
    "        query_embedding = await embedding_model.get_embeddings([query])\n",
    "        retrieved_chunks = await vector_store.get_chunks_by_embedding(\n",
    "            embedding=query_embedding, top_k=top_k\n",
    "        )\n",
    "\n",
    "        retrieved_chunks_as_text = chunks_to_text(retrieved_chunks)\n",
    "\n",
    "        return {\"result\": retrieved_chunks_as_text}\n",
    "\n",
    "\n",
    "alexnet_retriever_tool = RetrieveRelevantChunks(\n",
    "    name=\"retrieve_relevant_chunks\",\n",
    "    description=\"Retrieves the most relevant chunks from the AlexNet paper based on a query.\",\n",
    "    # What parameters it expects\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The query to retrieve relevant chunks for.\",\n",
    "            },\n",
    "            \"top_k\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The number of top relevant chunks to retrieve, maximum is 10.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64a852",
   "metadata": {},
   "source": [
    "# Setup RAG subagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0714eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:22:45.215 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.5; seed: 42; tools: [<__main__.RetrieveRelevantChunks object at 0x00000248592582F0>]; tool_choice: auto; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "class RAGAgent(ToolAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "llm = OpenAILLM(tool_choice=\"auto\", tools=[alexnet_retriever_tool])\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"You are a helpful assistant, answer shortly. Use the tools only when they are relevant, but if you do so, trust the results from the tools and use them in your answer, cite them precisely if you use them.\"\n",
    "prompt_as_message = LLMMessage(content=prompt, role=Roles.SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0e58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = RAGAgent(system_prompt=prompt, llm=llm, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cafb05",
   "metadata": {},
   "source": [
    "# Define it as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a17347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgentAsTool(Tool):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        parameters: dict[str, Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.parameters = parameters\n",
    "\n",
    "    async def call(self, args: dict[str, Any]) -> dict[str, Any]:\n",
    "        query = args.get(\"query\")\n",
    "\n",
    "        answer = await rag_agent.answer(\n",
    "            query_with_context=QueryWithContext(query=query, history=[])\n",
    "        )\n",
    "        answer_as_text = str(answer.content)\n",
    "\n",
    "        return {\"result\": answer_as_text}\n",
    "\n",
    "\n",
    "rag_agent_as_tool_tool = RAGAgentAsTool(\n",
    "    name=\"rag_agent_as_tool\",\n",
    "    description=\"Uses the RAG agent to answer queries based on the AlexNet paper.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The query to retrieve relevant chunks for.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde3d567",
   "metadata": {},
   "source": [
    "# Define the Main Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd4f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:22:45.517 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.5; seed: 42; tools: [<__main__.RAGAgentAsTool object at 0x0000024B4CA50200>]; tool_choice: auto; response_format: {'type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "class MainAgent(ToolAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "llm_main_agent = OpenAILLM(tool_choice=\"auto\", tools=[rag_agent_as_tool_tool])\n",
    "\n",
    "# Define the prompt, no need to explain the sources\n",
    "prompt_main_agent = \"You are a helpful assistant, answer shortly.\"\n",
    "prompt_as_message_main_agent = LLMMessage(content=prompt_main_agent, role=Roles.SYSTEM)\n",
    "\n",
    "main_agent = MainAgent(system_prompt=prompt_main_agent, llm=llm_main_agent, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311f62f",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f24cb6",
   "metadata": {},
   "source": [
    "## First Simple Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3dd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [prompt_as_message_main_agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e39d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:22:47.652 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': 'A CNN, or Convolutional Neural Network, is a type of deep learning model specifically designed for processing structured grid data, such as images. It utilizes convolutional layers to automatically detect and learn features from input data, making it particularly effective for tasks like image recognition, classification, and object detection. CNNs can capture spatial hierarchies in data through their layered architecture, which typically includes convolutional layers, pooling layers, and fully connected layers.', 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a CNN?\"\n",
    "query_as_message = LLMMessage(content=query, role=Roles.USER)\n",
    "query_with_context = QueryWithContext(query=query, history=[])\n",
    "\n",
    "answer = await main_agent.answer(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2c96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CNN, or Convolutional Neural Network, is a type of deep learning model specifically designed for processing structured grid data, such as images. It utilizes convolutional layers to automatically detect and learn features from input data, making it particularly effective for tasks like image recognition, classification, and object detection. CNNs can capture spatial hierarchies in data through their layered architecture, which typically includes convolutional layers, pooling layers, and fully connected layers.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5c26417",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation += [query_as_message, answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49762de0",
   "metadata": {},
   "source": [
    "## Follow Up about AlexNet\n",
    "\n",
    "And test remembers conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b95351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 15:22:51.446 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n",
      "2026-02-26 15:22:53.346 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': '', 'tool_calls': [ToolCall(id='call_BlRYBhLq1KFfm0sfqhMKnESL', function=Function(name='retrieve_relevant_chunks', arguments='{\"query\":\"top-1 and top-5 scores ILSVRC-2010 AlexNet\"}'), type='function')], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}, {'result': \"## Chunk 67:\\n```\\nOur results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0% 5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\\n```\\n------------------------------\\n\\n## Chunk 15:\\n```\\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.\\n```\\n------------------------------\\n\\n## Chunk 5:\\n```\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called 'dropout' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\n```\\n------------------------------\\n\\n## Chunk 69:\\n```\\nTable 1: Comparison of results on ILSVRC2010 test set. In italics are best results achieved by others.\\n```\\n------------------------------\\n\\n## Chunk 74:\\n```\\n| Model          | Top-1 (val)   | Top-5 (val)   | Top-5 (test)   | |----------------|---------------|---------------|----------------| | SIFT + FVs [7] | -             | -             | 26.2%          | | 1 CNN          | 40.7%         | 18.2%         | -              | | 5 CNNs         | 38.1%         | 16.4%         | 16.4%          | | 1 CNN*         | 39.0%         | 16.6%         | -              | | 7 CNNs*        | 36.7%         | 15.4%         | 15.3%          |\\n```\\n----------------------------\", 'role': 'tool', 'function_name': 'retrieve_relevant_chunks'}, {'content': 'AlexNet achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, on the ILSVRC-2010 test set. This performance was significantly better than the previous state-of-the-art results at that time.', 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n",
      "2026-02-26 15:22:54.901 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (1, 1024)\n",
      "2026-02-26 15:22:59.885 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': '', 'tool_calls': [ToolCall(id='call_dPOm4NpLyjXcNQfEKg1lFZE2', function=Function(name='retrieve_relevant_chunks', arguments='{\"query\":\"relationship between AlexNet and CNN\"}'), type='function')], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}, {'result': \"## Chunk 37:\\n```\\nNow we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.\\n```\\n------------------------------\\n\\n## Chunk 22:\\n```\\nWe are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al. [11] claim that the nonlinearity f ( x ) = | tanh( x ) | works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.\\n```\\n------------------------------\\n\\n## Chunk 8:\\n```\\nTo learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don't have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.\\n```\\n------------------------------\\n\\n## Chunk 41:\\n```\\nFigure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network's input is 150,528-dimensional, and the number of neurons in the network's remaining layers is given by 253,440-186,624-64,896-64,896-43,2644096-4096-1000.\\n```\\n------------------------------\\n\\n## Chunk 5:\\n```\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called 'dropout' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\n```\\n----------------------------\", 'role': 'tool', 'function_name': 'retrieve_relevant_chunks'}, {'content': \"AlexNet is a specific implementation of a Convolutional Neural Network (CNN) designed for image classification tasks. It consists of eight layers with weights, including five convolutional layers and three fully connected layers. AlexNet significantly advanced the field of deep learning by achieving state-of-the-art performance on the ImageNet dataset, demonstrating the effectiveness of deep CNNs for complex visual recognition tasks.\\n\\nKey features of AlexNet include:\\n\\n1. **Architecture**: It uses multiple convolutional layers followed by pooling layers, which help in extracting features from images.\\n2. **Learning Capacity**: AlexNet's architecture allows it to learn from a large dataset (1.2 million images) and classify them into 1000 categories, showcasing CNNs' ability to handle complex tasks with fewer parameters compared to traditional neural networks.\\n3. **Innovative Techniques**: It employed techniques like ReLU activation functions and dropout for regularization, which improved training efficiency and reduced overfitting.\\n\\nOverall, AlexNet exemplifies the capabilities of CNNs in deep learning, particularly for image-related tasks.\", 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n",
      "2026-02-26 15:23:05.787 | DEBUG    | conversational_toolkit.agents.tool_agent:answer_stream:106 - [{'content': '', 'tool_calls': [ToolCall(id='call_mK56nujyrny6YO1srY6o1bDy', function=Function(name='rag_agent_as_tool', arguments='{\"query\": \"top-1 and top-5 scores obtained on ILSVRC-2010 by AlexNet\"}'), type='function'), ToolCall(id='call_mfMlz6pReFQDbHDhCtdhTU8C', function=Function(name='rag_agent_as_tool', arguments='{\"query\": \"relationship between AlexNet and CNN\"}'), type='function')], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}, {'result': 'AlexNet achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, on the ILSVRC-2010 test set. This performance was significantly better than the previous state-of-the-art results at that time.', 'role': 'tool', 'function_name': 'rag_agent_as_tool'}, {'result': \"AlexNet is a specific implementation of a Convolutional Neural Network (CNN) designed for image classification tasks. It consists of eight layers with weights, including five convolutional layers and three fully connected layers. AlexNet significantly advanced the field of deep learning by achieving state-of-the-art performance on the ImageNet dataset, demonstrating the effectiveness of deep CNNs for complex visual recognition tasks.\\n\\nKey features of AlexNet include:\\n\\n1. **Architecture**: It uses multiple convolutional layers followed by pooling layers, which help in extracting features from images.\\n2. **Learning Capacity**: AlexNet's architecture allows it to learn from a large dataset (1.2 million images) and classify them into 1000 categories, showcasing CNNs' ability to handle complex tasks with fewer parameters compared to traditional neural networks.\\n3. **Innovative Techniques**: It employed techniques like ReLU activation functions and dropout for regularization, which improved training efficiency and reduced overfitting.\\n\\nOverall, AlexNet exemplifies the capabilities of CNNs in deep learning, particularly for image-related tasks.\", 'role': 'tool', 'function_name': 'rag_agent_as_tool'}, {'content': 'AlexNet achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, on the ILSVRC-2010 test set, significantly outperforming previous models at that time.\\n\\nAlexNet is a specific implementation of a Convolutional Neural Network (CNN) designed for image classification. It consists of multiple convolutional and fully connected layers, showcasing the effectiveness of deep CNNs in handling complex visual recognition tasks. Its architecture and innovative techniques advanced the field of deep learning significantly.', 'tool_calls': [], 'role': <Roles.ASSISTANT: 'assistant'>, 'function_name': 'llm'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top-1 and top-5 scores obtained on 'ILSVRC-2010' by AlexNet? How does AlexNet relate to my first question? Answer to both in few sentences.\"\n",
    "query_as_message = LLMMessage(content=query, role=Roles.USER)\n",
    "query_with_context = QueryWithContext(query=query, history=conversation)\n",
    "\n",
    "answer = await main_agent.answer(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919803f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, on the ILSVRC-2010 test set, significantly outperforming previous models at that time.\n",
      "\n",
      "AlexNet is a specific implementation of a Convolutional Neural Network (CNN) designed for image classification. It consists of multiple convolutional and fully connected layers, showcasing the effectiveness of deep CNNs in handling complex visual recognition tasks. Its architecture and innovative techniques advanced the field of deep learning significantly.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fa045",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
